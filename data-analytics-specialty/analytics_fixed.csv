question,ans1,ans2,ans3,ans4,ans5,ans6,correct_ans,check_radio,topic,domain,explanation
,A. Store the contents of the mapping file in an Amazon DynamoDB tabl,B. Preprocess the records as they arrive in the Kinesis Data Analytics application with an AWS Lambda function that fetches the mapping and supplements each record to include the territory code  if one exist,C. Change the SQL query in the application to include the new field in the SELECT statement.,D. Store the mapping file in an Amazon S3 bucket and configure the reference data column headers for the.csv file in the Kinesis Data Analytics applicatio,E. Change the SQL query in the application to include a join to the file’s S3 Amazon Resource Name (ARN)  and add the territory code field to the SELECT columns.,F. Store the mapping file in an Amazon S3 bucket and configure it as a reference data source for the Kinesis Data Analytics applicatio G. Change the SQL query in the application to include a join to the reference table and add the territory code field to the SELECT columns. H. Store the contents of the mapping file in an Amazon DynamoDB tabl I. Change the Kinesis Data Analytics application to send its output to an AWS Lambda function that fetches the mapping and supplements each record to include the territory code  if one exist J. Forward the record from the Lambda function to the original application destination.,3,radio,,,
A retail company leverages Amazon Athena for ad-hoc queries against an AWS Glue Data Catalog. The data analytics team manages the data catalog and data access for the company. The data analytics team wants to separate queries and manage the cost of running those queries by different workloads and teams. Ideally  the data analysts want to group the queries run by different users within a team  store the query results in individual Amazon S3 buckets specific to each team  and enforce cost constraints on the queries run against the Data Catalog. Which solution meets these requirements?,A. Create IAM groups and resource tags for each team within the compan,B. Set up IAM policies that controluser access and actions on the Data Catalog resources.,C. Create Athena resource groups for each team within the company and assign users to these group,D. Add S3 bucket names and other query configurations to the properties list for the resource groups.,E. Create Athena workgroups for each team within the compan,F. Set up IAM workgroup policies that control user access and actions on the workgroup resources. G. Create Athena query groups for each team within the company and assign users to the groups.,3,radio,,,https://aws.amazon.com/about-aws/whats-new/2019/02/athena_workgroups/
"A utility company wants to visualize data for energy usage on a daily basis in Amazon QuickSight A data analytics specialist at the company has built a data pipeline to collect and ingest the data into Amazon S3 Each day the data is stored in an individual csv file in an S3 bucket This is an example of the naming structure 20210707_datacsv 20210708_datacsv To allow for data querying in QuickSight through Amazon Athena the specialist used an AWS Glue crawler to create a table with the path ""s3 //powertransformer/20210707_data csv"" However when the data is queried  it returns zero rows How can this issue be resolved?",A. Modify the IAM policy for the AWS Glue crawler to access Amazon S3.,B. Ingest the files again.,C. Store the files in Apache Parquet format.,"D. Update the table path to ""s3://powertransformer/"".",,,4,radio,,,
A company’s data analyst needs to ensure that queries executed in Amazon Athena cannot scan more than a prescribed amount of data for cost control purposes. Queries that exceed the prescribed threshold must be canceled immediately. What should the data analyst do to achieve this?,A. Configure Athena to invoke an AWS Lambda function that terminates queries when the prescribed threshold is crossed.,B. For each workgroup  set the control limit for each query to the prescribed threshold.,C. Enforce the prescribed threshold on all Amazon S3 bucket policies,D. For each workgroup  set the workgroup-wide data usage control limit to the prescribed threshold.,,,2,radio,,,https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html
Once a month  a company receives a 100 MB .csv file compressed with gzip. The file contains 50 000 property listing records and is stored in Amazon S3 Glacier. The company needs its data analyst to query a subset of the data for a specific vendor. What is the most cost-effective solution? Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),A. Load the data into Amazon S3 and query it with Amazon S3 Select.,B. Query the data from Amazon S3 Glacier directly with Amazon Glacier Select.,C. Load the data to Amazon S3 and query it with Amazon Athena.,D. Load the data to Amazon S3 and query it with Amazon Redshift Spectrum.,,,1,radio,,,
A global company has different sub-organizations  and each sub-organization sells its products and services in various countries. The company's senior leadership wants to quickly identify which sub-organization is the strongest performer in each country. All sales data is stored in Amazon S3 in Parquet format. Which approach can provide the visuals that senior leadership requested with the least amount of effort?,A. Use Amazon QuickSight with Amazon Athena as the data sourc,B. Use heat maps as the visual type.,C. Use Amazon QuickSight with Amazon S3 as the data sourc,D. Use heat maps as the visual type.,E. Use Amazon QuickSight with Amazon Athena as the data sourc,F. Use pivot tables as the visual type. G. Use Amazon QuickSight with Amazon S3 as the data sourc H. Use pivot tables as the visual type.,1,radio,,,
A transport company wants to track vehicular movements by capturing geolocation records. The records are 10 B in size and up to 10 000 records are captured each second. Data transmission delays of a few minutes are acceptable  considering unreliable network conditions. The transport company decided to use Amazon Kinesis Data Streams to ingest the data. The company is looking for a reliable mechanism to send data to Kinesis Data Streams while maximizing the throughput efficiency of the Kinesis shards. Which solution will meet the company’s requirements?,A. Kinesis Agent,B. Kinesis Producer Library (KPL),C. Kinesis Data Firehose,D. Kinesis SDK,,,2,radio,,,
A data analyst is designing a solution to interactively query datasets with SQL using a JDBC connection. Users will join data stored in Amazon S3 in Apache ORC format with data stored in Amazon Elasticsearch Service (Amazon ES) and Amazon Aurora MySQL. Which solution will provide the MOST up-to-date results?,A. Use AWS Glue jobs to ETL data from Amazon ES and Aurora MySQL to Amazon S3. Query the data with Amazon Athena.,B. Use Amazon DMS to stream data from Amazon ES and Aurora MySQL to Amazon Redshif,C. Query the data with Amazon Redshift.,D. Query all the datasets in place with Apache Spark SQL running on an AWS Glue developer endpoint.,E. Query all the datasets in place with Apache Presto running on Amazon EMR.,,3,radio,,,
A company wants to run analytics on its Elastic Load Balancing logs stored in Amazon S3. A data analyst needs to be able to query all data from a desired year month  or day. The data analyst should also be able to query a subset of the columns. The company requires minimal operational overhead and the most costeffective solution. Which approach meets these requirements for optimizing and querying the log data?,A. Use an AWS Glue job nightly to transform new log files into .csv format and partition by year  month  and da,B. Use AWS Glue crawlers to detect new partition,C. Use Amazon Athena to query data.,D. Launch a long-running Amazon EMR cluster that continuously transforms new log files from Amazon S3 into its Hadoop Distributed File System (HDFS) storage and partitions by year  month  and da,E. Use Apache Presto to query the optimized format.,F. Launch a transient Amazon EMR cluster nightly to transform new log files into Apache ORC format and partition by year  month  and da G. Use Amazon Redshift Spectrum to query the data. H. Use an AWS Glue job nightly to transform new log files into Apache Parquet format and partition by year  month  and da I. Use AWS Glue crawlers to detect new partition J. Use Amazon Athena to querydata.,3,radio,,,
A large university has adopted a strategic goal of increasing diversity among enrolled students. The data analytics team is creating a dashboard with data visualizations to enable stakeholders to view historical trends. All access must be authenticated using Microsoft Active Directory. All data in transit and at rest must be encrypted. Which solution meets these requirements?,A. Amazon QuickSight Standard edition configured to perform identity federation using SAML 2.0. and the default encryption settings.,B. Amazon QuickSight Enterprise edition configured to perform identity federation using SAML 2.0 and the default encryption settings. Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),C. Amazon QuckSight Standard edition using AD Connector to authenticate using Active Directory.Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS.,D. Amazon QuickSight Enterprise edition using AD Connector to authenticate using Active Directory.Configure Amazon QuickSight to use customer-provided keys imported into AWS KMS.,,,4,radio,,,
A company has developed several AWS Glue jobs to validate and transform its data from Amazon S3 and load it into Amazon RDS for MySQL in batches once every day. The ETL jobs read the S3 data using a DynamicFrame. Currently  the ETL developers are experiencing challenges in processing only the incremental data on every run  as the AWS Glue job processes all the S3 input data on each run. Which approach would allow the developers to solve the issue with minimal coding effort?,A. Have the ETL jobs read the data from Amazon S3 using a DataFrame.,B. Enable job bookmarks on the AWS Glue jobs.,C. Create custom logic on the ETL jobs to track the processed S3 objects.,D. Have the ETL jobs delete the processed objects or data from Amazon S3 after each run.,,,2,radio,,,
A banking company wants to collect large volumes of transactional data using Amazon Kinesis Data Streams for real-time analytics. The company uses PutRecord to send data to Amazon Kinesis  and has observed network outages during certain times of the day. The company wants to obtain exactly once semantics for the entire processing pipeline. What should the company do to obtain these characteristics?,A. Design the application so it can remove duplicates during processing be embedding a unique ID in each record.,B. Rely on the processing semantics of Amazon Kinesis Data Analytics to avoid duplicate processing of events.,C. Design the data producer so events are not ingested into Kinesis Data Streams multiple times.,D. Rely on the exactly one processing semantics of Apache Flink and Apache Spark Streaming included in Amazon EMR.,,,1,radio,,,
A company uses Amazon Redshift for its data warehousing needs. ETL jobs run every night to load data  apply business rules  and create aggregate tables for reporting. The company's data analysis  data science  and business intelligence teams use the data warehouse during regular business hours. The workload management is set to auto  and separate queues exist for each team with the priority set to NORMAL. Recently  a sudden spike of read queries from the data analysis team has occurred at least twice daily  and queries wait in line for cluster resources. The company needs a solution that enables the data analysis team to avoid query queuing without impacting latency and the query times of other teams. Which solution meets these requirements?,A. Increase the query priority to HIGHEST for the data analysis queue.,B. Configure the data analysis queue to enable concurrency scaling.,C. Create a query monitoring rule to add more cluster capacity for the data analysis queue when queries are waiting for resources.,D. Use workload management query queue hopping to route the query to the next matching queue.,,,4,radio,,,
A company has developed an Apache Hive script to batch process data stared in Amazon S3. The script needs to run once every day and store the output in Amazon S3. The company tested the script  and it completes within 30 minutes on a small local three-node cluster. Which solution is the MOST cost-effective for scheduling and executing the script?,A. Create an AWS Lambda function to spin up an Amazon EMR cluster with a Hive execution ste,B. Set KeepJobFlowAliveWhenNoSteps to false and disable the termination protection fla,C. Use Amazon CloudWatch Events to schedule the Lambda function to run daily.,D. Use the AWS Management Console to spin up an Amazon EMR cluster with Python Hu,E. Hive  and Apache Oozi,F. Set the termination protection flag to true and use Spot Instances for the core nodes of the cluste G. Configure an Oozie workflow in the cluster to invoke the Hive script daily. H. Create an AWS Glue job with the Hive script to perform the batch operatio I. Configure the job to run once a day using a time-based schedule. J. Use AWS Lambda layers and load the Hive runtime to AWS Lambda and copy the Hive script.Schedule the Lambda function to run daily by creating a workflow using AWS Step Functions.,3,radio,,,
A marketing company collects clickstream data The company sends the data to Amazon Kinesis Data Firehose and stores the data in Amazon S3 The company wants to build a series of dashboards that will be used by hundreds of users across different departments The company will use Amazon QuickSight to develop these dashboards The company has limited resources and wants a solution that could scale and provide daily updates about clickstream activity Which combination of options will provide the MOST cost-effective solution? (Select TWO ),A. Use Amazon Redshift to store and query the clickstream data,B. Use QuickSight with a direct SQL query,C. Use Amazon Athena to query the clickstream data in Amazon S3,D. Use S3 analytics to query the clickstream data,E. Use the QuickSight SPICE engine with a daily refresh Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),,2,radio,,,
A media company wants to perform machine learning and analytics on the data residing in its Amazon S3 data lake. There are two data transformation requirements that will enable the consumers within the company to create reports: Daily transformations of 300 GB of data with different file formats landing in Amazon S3 at a scheduled time. One-time transformations of terabytes of archived data residing in the S3 data lake. Which combination of solutions cost-effectively meets the company’s requirements for transforming the data? (Choose three.),A. For daily incoming data  use AWS Glue crawlers to scan and identify the schema.,B. For daily incoming data  use Amazon Athena to scan and identify the schema.,C. For daily incoming data  use Amazon Redshift to perform transformations.,D. For daily incoming data  use AWS Glue workflows with AWS Glue jobs to perform transformations.,E. For archived data  use Amazon EMR to perform data transformations.,F. For archived data  use Amazon SageMaker to perform data transformations.,1,radio,,,
A company is planning to do a proof of concept for a machine learning (ML) project using Amazon SageMaker with a subset of existing on-premises data hosted in the company’s 3 TB data warehouse. For part of the project  AWS Direct Connect is established and tested. To prepare the data for ML  data analysts are performing data curation. The data analysts want to perform multiple step  including mapping  dropping null fields  resolving choice  and splitting fields. The company needs the fastest solution to curate the data for this project. Which solution meets these requirements?,A. Ingest data into Amazon S3 using AWS DataSync and use Apache Spark scrips to curate the data in an Amazon EMR cluste,B. Store the curated data in Amazon S3 for ML processing.,C. Create custom ETL jobs on-premises to curate the dat,D. Use AWS DMS to ingest data into Amazon S3 for ML processing.,E. Ingest data into Amazon S3 using AWS DM,F. Use AWS Glue to perform data curation and store the data in Amazon S3 for ML processing. G. Take a full backup of the data store and ship the backup files using AWS Snowbal H. Upload Snowball data into Amazon S3 and schedule data curation jobs using AWS Batch to prepare the data for ML.,3,radio,,,
A smart home automation company must efficiently ingest and process messages from various connected devices and sensors. The majority of these messages are comprised of a large number of small files. These messages are ingested using Amazon Kinesis Data Streams and sent to Amazon S3 using a Kinesis data stream consumer application. The Amazon S3 message data is then passed through a processing pipeline built on Amazon EMR running scheduled PySpark jobs. The data platform team manages data processing and is concerned about the efficiency and cost of downstream data processing. They want to continue to use PySpark. Which solution improves the efficiency of the data processing jobs and is well architected?,A. Send the sensor and devices data directly to a Kinesis Data Firehose delivery stream to send the data to Amazon S3 with Apache Parquet record format conversion enable,B. Use Amazon EMR running PySpark to process the data in Amazon S3.,C. Set up an AWS Lambda function with a Python runtime environmen,D. Process individual Kinesis data stream messages from the connected devices and sensors using Lambda.,E. Launch an Amazon Redshift cluste,F. Copy the collected data from Amazon S3 to Amazon Redshift and move the data processing jobs from Amazon EMR to Amazon Redshift. G. Set up AWS Glue Python jobs to merge the small data files in Amazon S3 into larger files and transform them to Apache Parquet forma H. Migrate the downstream PySpark jobs from Amazon EMR to AWS Glue.,4,radio,,,https://aws.amazon.com/it/about-aws/whats-new/2020/04/aws-glue-now-supports-serverless-streaming-etl/
A company is streaming its high-volume billing data (100 MBps) to Amazon Kinesis Data Streams. A data analyst partitioned the data on account_id to ensure that all records belonging to an account go to the same Kinesis shard and order is maintained. While building a custom consumer using the Kinesis Java SDK  the data analyst notices that  sometimes  the messages arrive out of order for account_id. Upon further investigation  the data analyst discovers the messages that are out of order seem to be arriving from different shards for the same account_id and are seen when a stream resize runs. What is an explanation for this behavior and what is the solution?,A. There are multiple shards in a stream and order needs to be maintained in the shar,B. The data analyst needs to make sure there is only a single shard in the stream and no stream resize runs.,C. The hash key generation process for the records is not working correctl,D. The data analyst should generate an explicit hash key on the producer side so the records are directed to the appropriate shard accurately.,E. The records are not being received by Kinesis Data Streams in orde,F. The producer should use the PutRecords API call instead of the PutRecord API call with the SequenceNumberForOrdering parameter. G. The consumer is not processing the parent shard completely before processing the child shards after a stream resiz H. The data analyst should process the parent shard completely first before processing the child shards.,4,radio,,,Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As) https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-after-resharding.html the parent shards that remain after the reshard could still contain data that you haven't read yet that was added to the stream before the reshard. If you read data from the child shards before having read all data from the parent shards  you could read data for a particular hash key out of the order given by the data records' sequence numbers. Therefore  assuming that the order of the data is important  you should  after a reshard  always continue to read data from the parent shards until it is exhausted. Only then should you begin reading data from the child shards.
A manufacturing company has many loT devices in different facilities across the world The company is using Amazon Kinesis Data Streams to collect the data from the devices The company's operations team has started to observe many WnteThroughputExceeded exceptions The operations team determines that the reason is the number of records that are being written to certain shards The data contains device ID capture date measurement type  measurement value and facility ID The facility ID is used as the partition key Which action will resolve this issue?,A. Change the partition key from facility ID to a randomly generated key,B. Increase the number of shards,C. Archive the data on the producers' side,D. Change the partition key from facility ID to capture date,,,2,radio,,,
An ecommerce company ingests a large set of clickstream data in JSON format and stores the data in Amazon S3. Business analysts from multiple product divisions need to use Amazon Athena to analyze the data. The company's analytics team must design a solution to monitor the daily data usage for Athena by each product division. The solution also must produce a warning when a divisions exceeds its quota Which solution will meet these requirements with the LEAST operational overhead?,A. Use a CREATE TABLE AS SELECT (CTAS) statement to create separate tables for each product division Use AWS Budgets to track Athena usage Configure a threshold for the budget Use Amazon Simple Notification Service (Amazon SNS) to send notifications when thresholds are breached.,B. Create an AWS account for each division Provide cross-account access to an AWS Glue Data Catalog to all the account,C. Set an Amazon CloudWatch alarm to monitor Athena usag,D. Use Amazon Simple Notification Service (Amazon SNS) to send notifications.,E. Create an Athena workgroup for each division Configure a data usage control for each workgroup and a time period of 1 day Configure an action to send notifications to an Amazon Simple Notification Service(Amazon SNS) topic,F. Create an AWS account for each division Configure an AWS Glue Data Catalog in each account Set an Amazon CloudWatch alarm to monitor Athena usage Use Amazon Simple Notification Service (Amazon SNS) to send notifications.,3,radio,,,
A large company receives files from external parties in Amazon EC2 throughout the day. At the end of the day  the files are combined into a single file compressed into a gzip file  and uploaded to Amazon S3. The total size of all the files is close to 100 GB daily. Once the files are uploaded to Amazon S3  an AWS Batch program executes a COPY command to load the files into an Amazon Redshift cluster. Which program modification will accelerate the COPY process?,A. Upload the individual files to Amazon S3 and run the COPY command as soon as the files become available.,B. Split the number of files so they are equal to a multiple of the number of slices in the Amazon Redshift cluste,C. Gzip and upload the files to Amazon S3. Run the COPY command on the files.,D. Split the number of files so they are equal to a multiple of the number of compute nodes in the Amazon Redshift cluste,E. Gzip and upload the files to Amazon S3. Run the COPY command on the files.,F. Apply sharding by breaking up the files so the distkey columns with the same values go to the same file.Gzip and upload the sharded files to Amazon S3. Run the COPY command on the files.,2,radio,,,
A company has an application that uses the Amazon Kinesis Client Library (KCL) to read records from a Kinesis data stream. After a successful marketing campaign  the application experienced a significant increase in usage. As a result  a data analyst had to split some shards in the data stream. When the shards were split  the application started throwing an ExpiredIteratorExceptions error sporadically. What should the data analyst do to resolve this?,A. Increase the number of threads that process the stream records.,B. Increase the provisioned read capacity units assigned to the stream’s Amazon DynamoDB table.,C. Increase the provisioned write capacity units assigned to the stream’s Amazon DynamoDB table.,D. Decrease the provisioned write capacity units assigned to the stream’s Amazon DynamoDB table.,,,3,radio,,,
A company is building a data lake and needs to ingest data from a relational database that has time-series data. The company wants to use managed services to accomplish this. The process needs to be scheduled daily and bring incremental data only from the source into Amazon S3. What is the MOST cost-effective approach to meet these requirements?,A. Use AWS Glue to connect to the data source using JDBC Driver,B. Ingest incremental records only using job bookmarks.,C. Use AWS Glue to connect to the data source using JDBC Driver,D. Store the last updated key in an Amazon DynamoDB table and ingest the data using the updated key as a filter.,E. Use AWS Glue to connect to the data source using JDBC Drivers and ingest the entire datase Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),F. Use appropriate Apache Spark libraries to compare the dataset  and find the delta. G. Use AWS Glue to connect to the data source using JDBC Drivers and ingest the full dat H. Use AWS DataSync to ensure the delta only is written into Amazon S3.,1,radio,,,https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html
A marketing company has data in Salesforce  MySQL  and Amazon S3. The company wants to use data from these three locations and create mobile dashboards for its users. The company is unsure how it should create the dashboards and needs a solution with the least possible customization and coding. Which solution meets these requirements?,A. Use Amazon Athena federated queries to join the data source,B. Use Amazon QuickSight to generate the mobile dashboards.,C. Use AWS Lake Formation to migrate the data sources into Amazon S3. Use Amazon QuickSight to generate the mobile dashboards.,D. Use Amazon Redshift federated queries to join the data source,E. Use Amazon QuickSight to generate the mobile dashboards.,F. Use Amazon QuickSight to connect to the data sources and generate the mobile dashboards.,3,radio,,,
A telecommunications company is looking for an anomaly-detection solution to identify fraudulent calls. The company currently uses Amazon Kinesis to stream voice call records in a JSON format from its on-premises database to Amazon S3. The existing dataset contains voice call records with 200 columns. To detect fraudulent calls  the solution would need to look at 5 of these columns only. The company is interested in a cost-effective solution using AWS that requires minimal effort and experience in anomaly-detection algorithms. Which solution meets these requirements?,A. Use an AWS Glue job to transform the data from JSON to Apache Parque,B. Use AWS Glue crawlers todiscover the schema and build the AWS Glue Data Catalo,C. Use Amazon Athena to create a table with a subset of column,D. Use Amazon QuickSight to visualize the data and then use Amazon QuickSight machine learning-powered anomaly detection.,E. Use Kinesis Data Firehose to detect anomalies on a data stream from Kinesis by running SQL queries  which compute an anomaly score for all calls and store the output in Amazon RD,F. Use Amazon Athena to build a dataset and Amazon QuickSight to visualize the results. G. Use an AWS Glue job to transform the data from JSON to Apache Parque H. Use AWS Glue crawlers to discover the schema and build the AWS Glue Data Catalo I. Use Amazon SageMaker to build an anomaly detection model that can detect fraudulent calls by ingesting data from Amazon S3. J. Use Kinesis Data Analytics to detect anomalies on a data stream from Kinesis by running SQL queries  which compute an anomaly score for all call K. Connect Amazon QuickSight to Kinesis Data Analytics to visualize the anomaly scores.,1,radio,,,
An online retailer is rebuilding its inventory management system and inventory reordering system to automatically reorder products by using Amazon Kinesis Data Streams. The inventory management system uses the Kinesis Producer Library (KPL) to publish data to a stream. The inventory reordering system uses the Kinesis Client Library (KCL) to consume data from the stream. The stream has been configured to scale as needed. Just before production deployment  the retailer discovers that the inventory reordering system is receiving duplicated data. Which factors could be causing the duplicated data? (Choose two.),A. The producer has a network-related timeout.,B. The stream’s value for the IteratorAgeMilliseconds metric is too high.,C. There was a change in the number of shards  record processors  or both.,D. The AggregationEnabled configuration property was set to true.,E. The max_records configuration property was set to a number that is too high.,,2,radio,,,
A company is migrating its existing on-premises ETL jobs to Amazon EMR. The code consists of a series of jobs written in Java. The company needs to reduce overhead for the system administrators without changing the underlying code. Due to the sensitivity of the data  compliance requires that the company use root device volume encryption on all nodes in the cluster. Corporate standards require that environments be provisioned though AWS CloudFormation when possible. Which solution satisfies these requirements?,A. Install open-source Hadoop on Amazon EC2 instances with encrypted root device volume,B. Configure the cluster in the CloudFormation template.,C. Use a CloudFormation template to launch an EMR cluste,D. In the configuration section of the cluster  define a bootstrap action to enable TLS.,E. Create a custom AMI with encrypted root device volume,F. Configure Amazon EMR to use the custom AMI using the CustomAmild property in the CloudFormation template. G. Use a CloudFormation template to launch an EMR cluste H. In the configuration section of the cluster  define a bootstrap action to encrypt the root device volume of every node.,3,radio,,,
A company wants to improve the data load time of a sales data dashboard. Data has been collected as .csv files and stored within an Amazon S3 bucket that is partitioned by date. The data is then loaded to an Amazon Redshift data warehouse for frequent analysis. The data volume is up to 500 GB per day. Which solution will improve the data loading performance?,A. Compress .csv files and use an INSERT statement to ingest data into Amazon Redshift.,B. Split large .csv files  then use a COPY command to load data into Amazon Redshift.,C. Use Amazon Kinesis Data Firehose to ingest data into Amazon Redshift.,D. Load the .csv files in an unsorted key order and vacuum the table in Amazon Redshift.,,,2,radio,,,https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html
A company stores Apache Parquet-formatted files in Amazon S3 The company uses an AWS Glue Data Catalog to store the table metadata and Amazon Athena to query and analyze the data The tables have a large number of partitions The queries are only run on small subsets of data in the table A data analyst adds new time partitions into the table as new data arrives The data analyst has been asked to reduce the query runtime Which solution will provide the MOST reduction in the query runtime?,A. Convert the Parquet files to the csv file format..Then attempt to query the data again,B. Convert the Parquet files to the Apache ORC file forma,C. Then attempt to query the data again,D. Use partition projection to speed up the processing of the partitioned table,E. Add more partitions to be used over the tabl,F. Then filter over two partitions and put all columns in the WHERE clause,3,radio,,,
An education provider’s learning management system (LMS) is hosted in a 100 TB data lake that is built on Amazon S3. The provider’s LMS supports hundreds of schools. The provider wants to build an advanced analytics reporting platform using Amazon Redshift to handle complex queries with optimal performance. System users will query the most recent 4 months of data 95% of the time while 5% of the queries will leverage data from the previous 12 months. Which solution meets these requirements in the MOST cost-effective way?,A. Store the most recent 4 months of data in the Amazon Redshift cluste,B. Use Amazon Redshift Spectrum to query data in the data lak,C. Use S3 lifecycle management rules to store data from the previous 12 months in Amazon S3 Glacier storage.,D. Leverage DS2 nodes for the Amazon Redshift cluste,E. Migrate all data from Amazon S3 to Amazon Redshif,F. Decommission the data lake. G. Store the most recent 4 months of data in the Amazon Redshift cluste H. Use Amazon Redshift Spectrum to query data in the data lak I. Ensure the S3 Standard storage class is in use with objects in the data lake. J. Store the most recent 4 months of data in the Amazon Redshift cluste K. Use Amazon Redshift federated queries to join cluster data with the data lake to reduce cost L. Ensure the S3 Standard storage class is in use with objects in the data lake.,3,radio,,,
A company wants to improve user satisfaction for its smart home system by adding more features to its recommendation engine. Each sensor asynchronously pushes its nested JSON data into Amazon Kinesis Data Streams using the Kinesis Producer Library (KPL) in Java. Statistics from a set of failed sensors showed that  when a sensor is malfunctioning  its recorded data is not always sent to the cloud. The company needs a solution that offers near-real-time analytics on the data from the most updated sensors. Which solution enables the company to meet these Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As) requirements?,"A. Set the RecordMaxBufferedTime property of the KPL to ""1"" to disable the buffering on the sensor side.Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL scrip",B. Push the enriched data to a fleet of Kinesis data streams and enable the data transformation feature to flatten the JSON fil,C. Instantiate a dense storage Amazon Redshift cluster and use it as the destination for the Kinesis Data Firehose delivery stream.,D. Update the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Jav,E. Use Kinesis Data Analytics to enrich the data based on a company-developed anomaly detection SQL scrip,"F. Direct the output of KDA application to a Kinesis Data Firehose delivery stream  enable the data transformation feature to flatten the JSON file  and set the Kinesis Data Firehose destination to an Amazon Elasticsearch Service cluster. G. Set the RecordMaxBufferedTime property of the KPL to ""0"" to disable the buffering on the sensor side.Connect for each stream a dedicated Kinesis Data Firehose delivery stream and enable the data transformation feature to flatten the JSON file before sending it to an Amazon S3 bucke H. Load the S3 data into an Amazon Redshift cluster. I. Update the sensors code to use the PutRecord/PutRecords call from the Kinesis Data Streams API with the AWS SDK for Jav J. Use AWS Glue to fetch and process data from the stream using the Kinesis Client Library (KCL). Instantiate an Amazon Elasticsearch Service cluster and use AWS Lambda to directly push data into it.",2,radio,,,https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html The KPL can incur an additional processing delay of up to RecordMaxBufferedTime within the library (user-configurable). Larger values of RecordMaxBufferedTime results in higher packing efficiencies and better performance. Applications that cannot tolerate this additional delay may need to use the AWS SDK directly.
A company that monitors weather conditions from remote construction sites is setting up a solution to collect temperature data from the following two weather stations. Station A  which has 10 sensors Station B  which has five sensors These weather stations were placed by onsite subject-matter experts. Each sensor has a unique ID. The data collected from each sensor will be collected using Amazon Kinesis Data Streams. Based on the total incoming and outgoing data throughput  a single Amazon Kinesis data stream with two shards is created. Two partition keys are created based on the station names. During testing  there is a bottleneck on data coming from Station A  but not from Station B. Upon review  it is confirmed that the total stream throughput is still less than the allocated Kinesis Data Streams throughput. How can this bottleneck be resolved without increasing the overall cost and complexity of the solution  while retaining the data collection quality requirements?,D. The data collected from each sensor will be collected using Amazon Kinesis Data Streams. Based on the total incoming and outgoing data throughput  a single Amazon Kinesis data stream with two shards is created. Two partition keys are created based on the station names. During testing  there is a bottleneck on data coming from Station A  but not from Station,B. Upon review  it is confirmed that the total stream throughput is still less than the allocated Kinesis Data Streams throughput. How can this bottleneck be resolved without increasing the overall cost and complexity of the solution  while retaining the data collection quality requirements?,A. Increase the number of shards in Kinesis Data Streams to increase the level of parallelism.,B. Create a separate Kinesis data stream for Station A with two shards  and stream Station A sensor data to the new stream.,C. Modify the partition key to use the sensor ID instead of the station name.,D. Reduce the number of sensors in Station A from 10 to 5 sensors.,3,radio,,,"https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Splitting increases the number of shards in your stream and therefore increases the data capacity of the stream. Because you are charged on a per-shard basis,
An online retail company with millions of users around the globe wants to improve its ecommerce analytics capabilities. Currently, clickstream data is uploaded directly to Amazon S3 as compressed files. Several times each day, an application running on Amazon EC2 processes the data and makes search options and reports available for visualization by editors and marketers. The company wants to make website clicks and aggregated data available to editors and marketers in minutes to enable them to connect with users more effectively. Which options will help meet these requirements in the MOST efficient way? (Choose two.),Use Amazon Kinesis Data Firehose to upload compressed and batched clickstream records to Amazon Elasticsearch Service.,Upload clickstream records to Amazon S3 as compressed files, then use AWS Lambda to send data to Amazon Elasticsearch Service from Amazon S3.,Use Amazon Elasticsearch Service deployed on Amazon EC2 to aggregate, filter, and process the data. Refresh content performance dashboards in near-real time.,Use Kibana to aggregate, filter, and visualize the data stored in Amazon Elasticsearch Service.,Refresh content performance dashboards in near-real time.,Upload clickstream records from Amazon S3 to Amazon Kinesis Data Streams and use a Kinesis Data Streams consumer to send records to Amazon Elasticsearch Service.,1,radio,,,, 
A company wants to collect and process events data from different departments in near-real time. Before storing the data in Amazon S3  the company needs to clean the data by standardizing the format of the address and timestamp columns. The data varies in size based on the overall load at each particular point in time. Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As) A single data record can be 100 KB-10 MB. How should a data analytics specialist design the solution for data ingestion?,B. How should a data analytics specialist design the solution for data ingestion?,A. Use Amazon Kinesis Data Stream,B. Configure a stream for the raw dat,C. Use a Kinesis Agent to write data to the strea,D. Create an Amazon Kinesis Data Analytics application that reads data from the raw stream  cleanses it  and stores the output to Amazon S3.,E. Use Amazon Kinesis Data Firehos,F. Configure a Firehose delivery stream with a preprocessing AWS Lambda function for data cleansin G. Use a Kinesis Agent to write data to the delivery strea H. Configure Kinesis Data Firehose to deliver the data to Amazon S3. I. Use Amazon Managed Streaming for Apache Kafk J. Configure a topic for the raw dat K. Use a Kafka producer to write data to the topi L. Create an application on Amazon EC2 that reads data from the topic by using the Apache Kafka consumer API  cleanses the data  and writes to Amazon S3. M. Use Amazon Simple Queue Service (Amazon SQS). Configure an AWS Lambda function to read events from the SQS queue and upload the events to Amazon S3.,2,radio,,
A large financial company is running its ETL process. Part of this process is to move data from Amazon S3 into an Amazon Redshift cluster. The company wants to use the most cost-efficient method to load the dataset into Amazon Redshift. Which combination of steps would meet these requirements? (Choose two.),A. Use the COPY command with the manifest file to load data into Amazon Redshift.,B. Use S3DistCp to load files into Amazon Redshift.,C. Use temporary staging tables during the loading process.,D. Use the UNLOAD command to upload data into Amazon Redshift.,E. Use Amazon Redshift Spectrum to query files from Amazon S3.,,1,radio,,,
A manufacturing company wants to create an operational analytics dashboard to visualize metrics from equipment in near-real time. The company uses Amazon Kinesis Data Streams to stream the data to other applications. The dashboard must automatically refresh every 5 seconds. A data analytics specialist must design a solution that requires the least possible implementation effort. Which solution meets these requirements?,A. Use Amazon Kinesis Data Firehose to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard.,B. Use Apache Spark Streaming on Amazon EMR to read the data in near-real tim,C. Develop a custom application for the dashboard by using D3.js.,D. Use Amazon Kinesis Data Firehose to push the data into an Amazon Elasticsearch Service (Amazon ES) cluste,E. Visualize the data by using a Kibana dashboard.,F. Use AWS Glue streaming ETL to store the data in Amazon S3. Use Amazon QuickSight to build the dashboard.,2,radio,,,
A company has 1 million scanned documents stored as image files in Amazon S3. The documents contain typewritten application forms with information including the applicant first name  applicant last name  application date  application type  and application text. The company has developed a machine learning algorithm to extract the metadata values from the scanned documents. The company wants to allow internal data analysts to analyze and find applications using the applicant name  application date  or application text. The original images should also be downloadable. Cost control is secondary to query performance. Which solution organizes the images and metadata to drive insights while meeting the requirements?,A. For each image  use object tags to add the metadat,B. Use Amazon S3 Select to retrieve the files based on the applicant name and application date.,C. Index the metadata and the Amazon S3 location of the image file in Amazon Elasticsearch Service.Allow the data analysts to use Kibana to submit queries to the Elasticsearch cluster.,D. Store the metadata and the Amazon S3 location of the image file in an Amazon Redshift tabl,E. Allow the data analysts to run ad-hoc queries on the table.,F. Store the metadata and the Amazon S3 location of the image files in an Apache Parquet file in Amazon S3  and define a table in the AWS Glue Data Catalo G. Allow data analysts to use Amazon Athena to submit custom queries.,2,radio,,,https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents
A company has a marketing department and a finance department. The departments are storing data in Amazon S3 in their own AWS accounts in AWS Organizations. Both departments use AWS Lake Formation to catalog and secure their data. The departments have some databases and tables that share common names. The marketing department needs to securely access some tables from the finance department. Which two steps are required for this process? (Choose two.),A. The finance department grants Lake Formation permissions for the tables to the external account for the marketing department.,B. The finance department creates cross-account IAM permissions to the table for the marketing department role.,C. The marketing department creates an IAM role that has permissions to the Lake Formation tables. Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),,,,1,radio,,,Granting Lake Formation Permissions Creating an IAM role (AWS CLI)
A healthcare company uses AWS data and analytics tools to collect  ingest  and store electronic health record (EHR) data about its patients. The raw EHR data is stored in Amazon S3 in JSON format partitioned by hour  day  and year and is updated every hour. The company wants to maintain the data catalog and metadata in an AWS Glue Data Catalog to be able to access the data using Amazon Athena or Amazon Redshift Spectrum for analytics. When defining tables in the Data Catalog  the company has the following requirements: Choose the catalog table name and do not rely on the catalog table naming algorithm. Keep the table updated with new partitions loaded in the respective S3 bucket prefixes. Which solution meets these requirements with minimal effort?,A. Run an AWS Glue crawler that connects to one or more data stores  determines the data structures  and writes tables in the Data Catalog.,B. Use the AWS Glue console to manually create a table in the Data Catalog and schedule an AWS Lambda function to update the table partitions hourly.,C. Use the AWS Glue API CreateTable operation to create a table in the Data Catalo,D. Create an AWS Glue crawler and specify the table as the source.,E. Create an Apache Hive catalog in Amazon EMR with the table schema definition in Amazon S3  and update the table partition with a scheduled jo,F. Migrate the Hive catalog to the Data Catalog.,3,radio,,,Updating Manually Created Data Catalog Tables Using Crawlers: To do this  when you define a crawler  instead of specifying one or more data stores as the source of a crawl  you specify one or more existing Data Catalog tables. The crawler then crawls the data stores specified by the catalog tables. In this case  no new tables are created; instead  your manually created tables are updated.
A company wants to optimize the cost of its data and analytics platform. The company is ingesting a number of .c sv and JSON files in Amazon S3 from various data sources. Incoming data is expected to be 50 GB each day. The company is using Amazon Athena to query the raw data in Amazon S3 directly. Most queries aggregate data from the past 12 months  and data that is older than 5 years is infrequently queried. The typical query scans about 500 MB of data and is expected to return results in less than 1 minute. The raw data must be retained indefinitely for compliance requirements. Which solution meets the company’s requirements?,A. Use an AWS Glue ETL job to compress  partition  and convert the data into a columnar data forma,B. Use Athena to query the processed datase,C. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after object creation.Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation.,D. Use an AWS Glue ETL job to partition and convert the data into a row-based data forma,E. Use Athena to query the processed datase,F. Configure a lifecycle policy to move the data into the Amazon S3 Standard- Infrequent Access (S3 Standard-IA) storage class 5 years after object creatio G. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after object creation. H. Use an AWS Glue ETL job to compress  partition  and convert the data into a columnar data forma I. Use Athena to query the processed datase J. Configure a lifecycle policy to move the processed data into the Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accesse K. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier forlong-term archival 7 days after the last date the object was accessed. L. Use an AWS Glue ETL job to partition and convert the data into a row-based data forma M. Use Athena to query the processed datase N. Configure a lifecycle policy to move the data into the Amazon S3 Standard- Infrequent Access (S3 Standard-IA) storage class 5 years after the object was last accesse O. Configure a second lifecycle policy to move the raw data into Amazon S3 Glacier for long-term archival 7 days after the last date the object was accessed.,1,radio,,,
A data analyst is using Amazon QuickSight for data visualization across multiple datasets generated by applications. Each application stores files within a separate Amazon S3 bucket. AWS Glue Data Catalog is used as a central catalog across all application data in Amazon S3. A new application stores its data within a separate S3 bucket. After updating the catalog to include the new application data source  the data analyst created a new Amazon QuickSight data source from an Amazon Athena table  but the import into SPICE failed. How should the data analyst resolve the issue?,A. Edit the permissions for the AWS Glue Data Catalog from within the Amazon QuickSight console.,B. Edit the permissions for the new S3 bucket from within the Amazon QuickSight console.,C. Edit the permissions for the AWS Glue Data Catalog from within the AWS Glue console.,D. Edit the permissions for the new S3 bucket from within the S3 console.,,,2,radio,,,
A company is building an analytical solution that includes Amazon S3 as data lake storage and Amazon Redshift for data warehousing. The company wants to use Amazon Redshift Spectrum to query the data that is stored in Amazon S3. Which steps should the company take to improve performance when the company uses Amazon Redshift Spectrum to query the S3 data files? (Select THREE ) Use gzip compression with individual file sizes of 1-5 GB,A. Use a columnar storage file format,B. Partition the data based on the most common query predicates,C. Split the data into KB-sized files. Guaranteed success with Our exam guides visit - https://www.certshared.com  gCertshared now are offering 100% pass ensure AWS-Certified-Data-Analytics-Specialty dumps! https://www.certshared.com/exam/AWS-Certified-Data-Analytics-Specialty/ (157 Q&As),D. Keep all files about the same size.,E. Use file formats that are not splittable,,2,radio,,,
An ecommerce company is migrating its business intelligence environment from on premises to the AWS Cloud. The company will use Amazon Redshift in a public subnet and Amazon QuickSight. The tables already are loaded into Amazon Redshift and can be accessed by a SQL tool. The company starts QuickSight for the first time. During the creation of the data source  a data analytics specialist enters all the information and tries to validate the connection. An error with the following message occurs: “Creating a connection to your data source timed out.” How should the data analytics specialist resolve this error?,A. Grant the SELECT permission on Amazon Redshift tables.,B. Add the QuickSight IP address range into the Amazon Redshift security group.,C. Create an IAM role for QuickSight to access Amazon Redshift.,D. Use a QuickSight admin user for creating the dataset.,,,1,radio,,,Connection to the database times out Your client connection to the database appears to hang or time out when running long queries  such as a COPY command. In this case  you might observe that the Amazon Redshift console displays that the query has completed  but the client tool itself still appears to be running the query. The results of the query might be missing or incomplete depending on when the connection stopped.
A financial company uses Amazon S3 as its data lake and has set up a data warehouse using a multi-node Amazon Redshift cluster. The data files in the data lake are organized in folders based on the data source of each data file. All the data files are loaded to one table in the Amazon Redshift cluster using a separate COPY command for each data file location. With this approach  loading all the data files into Amazon Redshift takes a long time to complete. Users want a faster solution with little or no increase in cost while maintaining the segregation of the data files in the S3 data lake. Which solution meets these requirements?,A. Use Amazon EMR to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift.,B. Load all the data files in parallel to Amazon Aurora  and run an AWS Glue job to load the data into Amazon Redshift.,C. Use an AWS Glue job to copy all the data files into one folder and issue a COPY command to load the data into Amazon Redshift.,D. Create a manifest file that contains the data file locations and issue a COPY command to load the data into Amazon Redshift.,,,4,radio,,,"https://docs.aws.amazon.com/redshift/latest/dg/loading-data-files-using-manifest.html ""You can use a manifest to ensure that the COPY command loads all of the required files  and only the required files  for a data load"""
A market data company aggregates external data sources to create a detailed view of product consumption in different countries. The company wants to sell this data to external parties through a subscription. To achieve this goal  the company needs to make its data securely available to external parties who are also AWS users. What should the company do to meet these requirements with the LEAST operational overhead?,A. Store the data in Amazon S3. Share the data by using presigned URLs for security.,B. Store the data in Amazon S3. Share the data by using S3 bucket ACLs.,C. Upload the data to AWS Data Exchange for storag,D. Share the data by using presigned URLs for security.,E. Upload the data to AWS Data Exchange for storag,F. Share the data by using the AWS Data Exchange sharing wizard.,1,radio,,,
A company hosts an Apache Flink application on premises. The application processes data from several Apache Kafka clusters. The data originates from a variety of sources  such as web applications mobile apps and operational databases The company has migrated some of these sources to AWS and now wants to migrate the Flink application. The company must ensure that data that resides in databases within the VPC does not traverse the internet The application must be able to process all the data that comes from the company's AWS solution  on-premises resources and the public internet Which solution will meet these requirements with the LEAST operational overhead?,A. Implement Flink on Amazon EC2 within the company's VPC Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the VPC to collect data that comes from applications and databases within the VPC Use Amazon Kinesis Data Streams to collect data that comes from the public internet Configure Flink to have sources from Kinesis Data Streams Amazon MSK and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect,B. Implement Flink on Amazon EC2 within the company's VPC Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet Configure Flink to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect,C. Create an Amazon Kinesis Data Analytics application by uploading the compiled Flink jar file Use Amazon Kinesis Data Streams to collect data that comes from applications and databases within the VPC and the public internet Configure the Kinesis Data Analytics application to have sources from Kinesis Data Streams and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect,D. Create an Amazon Kinesis Data Analytics application by uploading the compiled Flink jar file Create Amazon Managed Streaming for Apache Kafka (Amazon MSK) clusters in the company's VPC to collect data that comes from applications and databases within the VPC Use Amazon Kinesis Data Streams to collect data that comes from the public internet Configure the Kinesis Data Analytics application to have sources from Kinesis Data Stream,E. Amazon MSK and any on-premises Kafka clusters by using AWS Client VPN or AWS Direct Connect,,4,radio,,,
