NEW QUESTION 1
A company migrated one of its business-critical database workloads to an Amazon Aurora Multi-AZ DB cluster. The company requires a very low RTO and needs
to improve the application recovery time after database failovers.
Which approach meets these requirements?
A. Set the max_connections parameter to 16,000 in the instance-level parameter group.
B. Modify the client connection timeout to 300 seconds.
C. Create an Amazon RDS Proxy database proxy and update client connections to point to the proxy endpoint.
D. Enable the query cache at the instance level.
Answer: C
Explanation:
Amazon RDS Proxy allows applications to pool and share connections established with the database,
improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66% and database
credentials, authentication, and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM).
https://aws.amazon.com/rds/proxy/

NEW QUESTION 2
A corporation intends to migrate a 500-GB Oracle database to Amazon Aurora PostgreSQL utilizing the AWS Schema Conversion Tool (AWS SCT) and AWS
Data Management Service (AWS DMS). The database does not have any stored procedures, but does contain several huge or partitioned tables. Because the
program is vital to the company, it is preferable to migrate with little downtime.
Which measures should a database professional perform in combination to expedite the transfer process? (Select three.)
A. Use the AWS SCT data extraction agent to migrate the schema from Oracle to Aurora PostgreSQL.
B. For the large tables, change the setting for the maximum number of tables to load in parallel and perform a full load using AWS DMS.
C. For the large tables, create a table settings rule with a parallel load option in AWS DMS, then perform a full load using DMS.
D. Use AWS DMS to set up change data capture (CDC) for continuous replication until the cutover date.
E. Use AWS SCT to convert the schema from Oracle to Aurora PostgreSQL.
F. Use AWS DMS to convert the schema from Oracle to Aurora PostgreSQL and for continuous replication.
Answer: CDE

NEW QUESTION 3
A financial services organization employs an Amazon Aurora PostgreSQL DB cluster to host an application on AWS. No log files detailing database administrator
activity were discovered during a recent examination. A database professional must suggest a solution that enables access to the database and maintains activity
logs. The solution should be simple to implement and have a negligible effect on performance.
Which database specialist solution should be recommended?
A. Enable Aurora Database Activity Streams on the database in synchronous mod
B. Connect the Amazon Kinesis data stream to Kinesis Data Firehos
C. Set the Kinesis Data Firehose destination to an Amazon S3 bucket.
D. Create an AWS CloudTrail trail in the Region where the database run
E. Associate the database activity logs with the trail.
F. Enable Aurora Database Activity Streams on the database in asynchronous mod
G. Connect the Amazon Kinesis data stream to Kinesis Data Firehos
H. Set the Firehose destination to an Amazon S3 bucket.
I. Allow connections to the DB cluster through a bastion host onl
J. Restrict database access to the bastion host and application server
K. Push the bastion host logs to Amazon CloudWatch Logs using the CloudWatch Logs agent.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html

NEW QUESTION 4
A user has a non-relational key-value database. The user is looking for a fully managed AWS service that will offload the administrative burdens of operating and
scaling distributed databases. The solution must be cost- effective and able to handle unpredictable application traffic.
What should a Database Specialist recommend for this user?
A. Create an Amazon DynamoDB table with provisioned capacity mode
B. Create an Amazon DocumentDB cluster
C. Create an Amazon DynamoDB table with on-demand capacity mode
D. Create an Amazon Aurora Serverless DB cluster
Answer: C

NEW QUESTION 5
A company has an ecommerce web application with an Amazon RDS for MySQL DB instance. The marketing team has noticed some unexpected updates to the
product and pricing information on the website, which is impacting sales targets. The marketing team wants a database specialist to audit future database activity
to help identify how and when the changes are being made.
What should the database specialist do to meet these requirements? (Choose two.)
A. Create an RDS event subscription to the audit event type.
B. Enable auditing of CONNECT and QUERY_DML events.
C. SSH to the DB instance and review the database logs.
D. Publish the database logs to Amazon CloudWatch Logs.
E. Enable Enhanced Monitoring on the DB instance.
Answer: BD
Explanation:
https://aws.amazon.com/blogs/database/configuring-an-audit-log-to-capture-database-activities-for-amazon-rds

NEW QUESTION 6
Recently, a financial institution created a portfolio management service. The application's backend is powered by Amazon Aurora, which supports MySQL.
The firm demands a response time of five minutes and a response time of five minutes. A database professional must create a disaster recovery system that is
both efficient and has a low replication latency.
How should the database professional tackle these requirements?
A. Configure AWS Database Migration Service (AWS DMS) and create a replica in a different AWS Region.
B. Configure an Amazon Aurora global database and add a different AWS Region.
C. Configure a binlog and create a replica in a different AWS Region.
D. Configure a cross-Region read replica.
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.ht https://aws.amazon.com/blogs/database/how-tochoose-the-best-disaster-recovery-option-for-your-amazon-auro
https://aws.amazon.com/about-aws/whats-new/2019/11/aurora-supports-in-place-conversion-to-global-database/

NEW QUESTION 7
A Database Specialist is performing a proof of concept with Amazon Aurora using a small instance to confirm a simple database behavior. When loading a large
dataset and creating the index, the Database Specialist encounters the following error message from Aurora:
ERROR: cloud not write block 7507718 of temporary file: No space left on device
What is the cause of this error and what should the Database Specialist do to resolve this issue?
A. The scaling of Aurora storage cannot catch up with the data loadin
B. The Database Specialist needs to modify the workload to load the data slowly.
C. The scaling of Aurora storage cannot catch up with the data loadin
D. The Database Specialist needs to enable Aurora storage scaling.
E. The local storage used to store temporary tables is ful
F. The Database Specialist needs to scale up the instance.
G. The local storage used to store temporary tables is ful
H. The Database Specialist needs to enable localstorage scaling.
Answer: C

NEW QUESTION 8
A financial company wants to store sensitive user data in an Amazon Aurora PostgreSQL DB cluster. The database will be accessed by multiple applications
across the company. The company has mandated that all communications to the database be encrypted and the server identity must be validated. Any non-SSLbased connections should be disallowed access to the database.
Which solution addresses these requirements?
A. Set the rds.force_ssl=0 parameter in DB parameter group
B. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=allow.
C. Set the rds.force_ssl=1 parameter in DB parameter group
D. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=disable.
E. Set the rds.force_ssl=0 parameter in DB parameter group
F. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-ca.
G. Set the rds.force_ssl=1 parameter in DB parameter group
H. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-full.
Answer: D
Explanation:
PostgreSQL: sslrootcert=rds-cert.pem sslmode=[verify-ca | verify-full]

NEW QUESTION 9
A business that specializes in internet advertising is developing an application that will show adverts to its customers. The program stores data in an Amazon
DynamoDB database. Additionally, the application caches its reads using a DynamoDB Accelerator (DAX) cluster. The majority of reads come via the GetItem and
BatchGetItem queries. The application does not need consistency of readings.
The application cache does not behave as intended after deployment. Specific extremely consistent queries to the DAX cluster are responding in several
milliseconds rather than microseconds.
How can the business optimize cache behavior in order to boost application performance?
A. Increase the size of the DAX cluster.
B. Configure DAX to be an item cache with no query cache
C. Use eventually consistent reads instead of strongly consistent reads.
D. Create a new DAX cluster with a higher TTL for the item cache.
Answer: C

NEW QUESTION 10
A company has an AWS CloudFormation template written in JSON that is used to launch new Amazon RDS for MySQL DB instances. The security team has
asked a database specialist to ensure that the master password is automatically rotated every 30 days for all new DB instances that are launched using the
template.
What is the MOST operationally efficient solution to meet these requirements?
A. Save the password in an Amazon S3 objec
B. Encrypt the S3 object with an AWS KMS ke
C. Set the KMS key to be rotated every 30 days by setting the EnableKeyRotation property to tru
D. Use a CloudFormation custom resource to read the S3 object to extract the password.
E. Create an AWS Lambda function to rotate the secre
F. Modify the CloudFormation template to add an AWS::SecretsManager::RotationSchedule resourc
G. Configure the RotationLambdaARN value and, for the RotationRules property, set the AutomaticallyAfterDays parameter to 30.
H. Modify the CloudFormation template to use the AWS KMS key as the database passwor
I. Configure an Amazon EventBridge rule to invoke the KMS API to rotate the key every 30 days by setting the ScheduleExpression parameter to ***/30***.
J. Integrate the Amazon RDS for MySQL DB instances with AWS IAM and centrally manage the master database user password.
Answer: B
Explanation:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationsche

NEW QUESTION 10
A company is using 5 TB Amazon RDS DB instances and needs to maintain 5 years of monthly database backups for compliance purposes. A Database
Administrator must provide Auditors with data within 24 hours. Which solution will meet these requirements and is the MOST operationally efficient?
A. Create an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot.Move the snapshot to the company’s Amazon S3
bucket.
B. Create an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot.
C. Create an RDS snapshot schedule from the AWS Management Console to take a snapshot every 30days.
D. Create an AWS Lambda function to run on the first day of every month to create an automated RDS snapshot.
Answer: A
Explanation:
Unlike automated backups, manual snapshots aren't subject to the backup retention period. Snapshots don't expire. For very long-term backups of MariaDB,
MySQL, and PostgreSQL data, we recommend exporting snapshot data to Amazon S3. If the major version of your DB engine is no longer supported, you can't
restore to that version from a snapshot.

NEW QUESTION 11
A business need a data warehouse system that stores data consistently and in a highly organized fashion. The organization demands rapid response times for enduser inquiries including current-year data, and users must have access to the whole 15-year dataset when necessary. Additionally, this solution must be able to
manage a variable volume of incoming inquiries. Costs associated with storing the 100 TB of data must be maintained to a minimum.
Which solution satisfies these criteria?
A. Leverage an Amazon Redshift data warehouse solution using a dense storage instance type while keeping all the data on local Amazon Redshift storag
B. Provision enough instances to support high demand.
C. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat
D. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye
E. Provision enough instances to support high demand.
F. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat
G. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye
H. Enable Amazon Redshift Concurrency Scaling.
I. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat
J. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye
K. Leverage Amazon Redshift elastic resize.
Answer: C
Explanation:
"With the Concurrency Scaling feature, you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.
When concurrency scaling is enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read
queries. Write operations continue as normal on your main cluster. Users always see the most current data, whether the queries run on the main cluster or on a
concurrency scaling cluster. You're charged for concurrency scaling clusters only for the time they're in use. For more information about pricing, see Amazon
Redshift pricing. You manage which queries are sent to the concurrency scaling cluster by configuring WLM queues. When you enable concurrency scaling for a
queue, eligible queries are sent to the concurrency scaling cluster instead of waiting in line."

NEW QUESTION 15
A company wants to automate the creation of secure test databases with random credentials to be stored safely for later use. The credentials should have
sufficient information about each test database to initiate a connection and perform automated credential rotations. The credentials should not be logged or stored
anywhere in an unencrypted form.
Which steps should a Database Specialist take to meet these requirements using an AWS CloudFormation template?
A. Create the database with the MasterUserName and MasterUserPassword properties set to the default value
B. Then, create the secret with the user name and password set to the same default value
C. Add aSecret Target Attachment resource with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the databas
D. Finally, update the secret’s password value with a randomly generated string set by the GenerateSecretString property.
E. Add a Mapping property from the database Amazon Resource Name (ARN) to the secret AR
F. Then, create the secret with a chosen user name and a randomly generated password set by the GenerateSecretString propert
G. Add the database with the MasterUserName and MasterUserPassword properties set to the user name of the secret.
H. Add a resource of type AWS::SecretsManager::Secret and specify the GenerateSecretString property.Then, define the database user name in the
SecureStringTemplate templat
I. Create a resource for the database and reference the secret string for the MasterUserName and MasterUserPassword propertie
J. Then, add a resource of type AWS::SecretsManagerSecretTargetAttachment with the SecretId and TargetId properties set to the Amazon Resource Names
(ARNs) of the secret and the database.
K. Create the secret with a chosen user name and a randomly generated password set by the GenerateSecretString propert
L. Add an SecretTargetAttachment resource with the SecretId property set to the Amazon Resource Name (ARN) of the secret and the TargetId property set to a
parameter value matching the desired database AR
M. Then, create a database with the MasterUserName and MasterUserPassword properties set to the previously created values in the secret.
Answer: C

NEW QUESTION 16
Recently, an ecommerce business transferred one of its SQL Server databases to an Amazon RDS for SQL Server Enterprise Edition database instance. The
corporation anticipates an increase in read traffic as a result of an approaching sale. To accommodate the projected read load, a database professional must
establish a read replica of the database instance.
Which procedures should the database professional do prior to establishing the read replica? (Select two.)
A. Identify a potential downtime window and stop the application calls to the source DB instance.
B. Ensure that automatic backups are enabled for the source DB instance.
C. Ensure that the source DB instance is a Multi-AZ deployment with Always ON Availability Groups.
D. Ensure that the source DB instance is a Multi-AZ deployment with SQL Server Database Mirroring(DBM).
E. Modify the read replica parameter group setting and set the value to 1.
Answer: BC
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.ReadReplicas.html

NEW QUESTION 18
A business is operating an on-premises application that is divided into three tiers: web, application, and MySQL database. The database is predominantly
accessed during business hours, with occasional bursts of activity throughout the day. As part of the company's shift to AWS, a database expert wants to increase
the availability and minimize the cost of the MySQL database tier.
Which MySQL database choice satisfies these criteria?
A. Amazon RDS for MySQL with Multi-AZ
B. Amazon Aurora Serverless MySQL cluster
C. Amazon Aurora MySQL cluster
D. Amazon RDS for MySQL with read replica
Answer: B
Explanation:
Amazon Aurora Serverless v1 is a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.
https://aws.amazon.com/rds/aurora/serverless/

NEW QUESTION 19
A company is developing a new web application. An AWS CloudFormation template was created as a part of the build process.
Recently, a change was made to an AWS::RDS::DBInstance resource in the template. The CharacterSetName property was changed to allow the application to
process international text. A change set was generated using the new template, which indicated that the existing DB instance should be replaced during an
upgrade.
What should a database specialist do to prevent data loss during the stack upgrade?
A. Create a snapshot of the DB instanc
B. Modify the template to add the DBSnapshotIdentifier property with the ID of the DB snapsho
C. Update the stack.
D. Modify the stack policy using the aws cloudformation update-stack command and the set-stack-policy command, then make the DB resource protected.
E. Create a snapshot of the DB instanc
F. Update the stac
G. Restore the database to a new instance.
H. Deactivate any applications that are using the DB instanc
I. Create a snapshot of the DB instance.Modify the template to add the DBSnapshotIdentifier property with the ID of the DB snapsho
J. Update the stack and reactivate the applications.
Answer: D
Explanation:
To preserve your data, perform the following procedure:
* 1.Deactivate any applications that are using the DB instance so that there's no activity on the DB instance. * 2.Create a snapshot of the DB instance. For more
information about creating DB snapshots
* 3. If you want to restore your instance using a DB snapshot, modify the updated template with your DB instance changes and add the DBSnapshotIdentifier
property with the ID of the DB snapshot that you want to use
* 4. Update the stack.

NEW QUESTION 22
A marketing company is using Amazon DocumentDB and requires that database audit logs be enabled. A Database Specialist needs to configure monitoring so
that all data definition language (DDL) statements performed are visible to the Administrator. The Database Specialist has set the audit_logs parameter to enabled
in the cluster parameter group.
What should the Database Specialist do to automatically collect the database logs for the Administrator?
A. Enable DocumentDB to export the logs to Amazon CloudWatch Logs
B. Enable DocumentDB to export the logs to AWS CloudTrail
C. Enable DocumentDB Events to export the logs to Amazon CloudWatch Logs
D. Configure an AWS Lambda function to download the logs using the download-db-log-file-portion operation and store the logs in Amazon S3
Answer: C
Explanation:
When auditing is enabled, Amazon DocumentDB records Data Definition Language (DDL), authentication, authorization, and user management events to Amazon
CloudWatch Logs. When auditing is enabled, Amazon DocumentDB exports your cluster’s auditing records (JSON documents) to Amazon CloudWatch Logs. You
can use Amazon CloudWatch Logs to analyze, monitor, and archive your Amazon DocumentDB auditing events.

NEW QUESTION 26
A media company is using Amazon RDS for PostgreSQL to store user data. The RDS DB instance currently has a publicly accessible setting enabled and is
hosted in a public subnet. Following a recent AWS Well- Architected Framework review, a Database Specialist was given new security requirements.
Only certain on-premises corporate network IPs should connect to the DB instance. Connectivity is allowed from the corporate network only.
Which combination of steps does the Database Specialist need to take to meet these new requirements? (Choose three.)
A. Modify the pg_hba.conf fil
B. Add the required corporate network IPs and remove the unwanted IPs.
C. Modify the associated security grou
D. Add the required corporate network IPs and remove the unwanted IPs.
E. Move the DB instance to a private subnet using AWS DMS.
F. Enable VPC peering between the application host running on the corporate network and the VPC associated with the DB instance.
G. Disable the publicly accessible setting.
H. Connect to the DB instance using private IPs and a VPN.
Answer: BEF
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.ht

NEW QUESTION 27
The Security team for a finance company was notified of an internal security breach that happened 3 weeks ago. A Database Specialist must start producing audit
logs out of the production Amazon Aurora PostgreSQL cluster for the Security team to use for monitoring and alerting. The Security team is required to perform
real- time alerting and monitoring outside the Aurora DB cluster and wants to have the cluster push encrypted files to the chosen solution.
Which approach will meet these requirements?
A. Use pg_audit to generate audit logs and send the logs to the Security team.
B. Use AWS CloudTrail to audit the DB cluster and the Security team will get data from Amazon S3.
C. Set up database activity streams and connect the data stream from Amazon Kinesis to consumer applications.
D. Turn on verbose logging and set up a schedule for the logs to be dumped out for the Security team.
Answer: C
Explanation:
https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-aurora-with-postgresql-compatibility-supports- "Database Activity Streams for Amazon Aurora
with PostgreSQL compatibility provides a near real-time data stream of the database activity in your relational database to help you monitor activity. When
integrated with third party database activity monitoring tools, Database Activity Streams can monitor and audit database activity to provide safeguards for your
database and help meet compliance and regulatory requirements."
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Overview.LoggingAndMonitoring.html

NEW QUESTION 28
A company is concerned about the cost of a large-scale, transactional application using Amazon DynamoDB that only needs to store data for 2 days before it is
deleted. In looking at the tables, a Database Specialist notices that much of the data is months old, and goes back to when the application was first deployed.
What can the Database Specialist do to reduce the overall cost?
A. Create a new attribute in each table to track the expiration time and create an AWS Glue transformation to delete entries more than 2 days old.
B. Create a new attribute in each table to track the expiration time and enable DynamoDB Streams on each table.
C. Create a new attribute in each table to track the expiration time and enable time to live (TTL) on each table.
D. Create an Amazon CloudWatch Events event to export the data to Amazon S3 daily using AWS Data Pipeline and then truncate the Amazon DynamoDB table.
Answer: C
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html

NEW QUESTION 33
A database specialist is building a system that uses a static vendor dataset of postal codes and related territory information that is less than 1 GB in size. The
dataset is loaded into the application’s cache at start up. The company needs to store this data in a way that provides the lowest cost with a low application startup
time.
Which approach will meet these requirements?
A. Use an Amazon RDS DB instanc
B. Shut down the instance once the data has been read.
C. Use Amazon Aurora Serverles
D. Allow the service to spin resources up and down, as needed.
E. Use Amazon DynamoDB in on-demand capacity mode.
F. Use Amazon S3 and load the data from flat files.
Answer: D
Explanation:
https://www.sumologic.com/insight/s3-cost-optimization/
For example, for 1 GB file stored on S3 with 1 TB of storage provisioned, you are billed for 1 GB only. In a lot of other services such as Amazon EC2, Amazon
Elastic Block Storage (Amazon EBS) and Amazon DynamoDB you pay for provisioned capacity. For example, in the case of Amazon EBS disk you pay for the size
of 1 TB of disk even if you just save 1 GB file. This makes managing S3 cost easier than many other services including Amazon EBS and Amazon EC2. On S3
there is no risk of over-provisioning and no need to manage disk utilization.

NEW QUESTION 37
A company has an on-premises system that tracks various database operations that occur over the lifetime of a database, including database shutdown, deletion,
creation, and backup.
The company recently moved two databases to Amazon RDS and is looking at a solution that would satisfy these requirements. The data could be used by other
systems within the company.
Which solution will meet these requirements with minimal effort?
A. Create an Amazon Cloudwatch Events rule with the operations that need to be tracked on Amazon RD
B. Create an AWS Lambda function to act on these rules and write the output to the tracking systems.
C. Create an AWS Lambda function to trigger on AWS CloudTrail API call
D. Filter on specific RDS API calls and write the output to the tracking systems.
E. Create RDS event subscription
F. Have the tracking systems subscribe to specific RDS event system notifications.
G. Write RDS logs to Amazon Kinesis Data Firehos
H. Create an AWS Lambda function to act on these rules and write the output to the tracking systems.
Answer: C

NEW QUESTION 41
A company is running its line of business application on AWS, which uses Amazon RDS for MySQL at the persistent data store. The company wants to minimize
downtime when it migrates the database to Amazon Aurora.
Which migration method should a Database Specialist use?
A. Take a snapshot of the RDS for MySQL DB instance and create a new Aurora DB cluster with the option to migrate snapshots.
B. Make a backup of the RDS for MySQL DB instance using the mysqldump utility, create a new Aurora DB cluster, and restore the backup.
C. Create an Aurora Replica from the RDS for MySQL DB instance and promote the Aurora DB cluster.
D. Create a clone of the RDS for MySQL DB instance and promote the Aurora DB cluster.
Answer: C
Explanation:
https://aws.amazon.com/blogs/database/best-practices-for-migrating-rds-for-mysql-databases-to-amazon-aurora/
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#Aurora

NEW QUESTION 43
A company has a heterogeneous six-node production Amazon Aurora DB cluster that handles online transaction processing (OLTP) for the core business and
OLAP reports for the human resources department. To match compute resources to the use case, the company has decided to have the reporting workload for the
human resources department be directed to two small nodes in the Aurora DB cluster, while every other workload goes to four large nodes in the same DB cluster.
Which option would ensure that the correct nodes are always available for the appropriate workload while meeting these requirements?
A. Use the writer endpoint for OLTP and the reader endpoint for the OLAP reporting workload.
B. Use automatic scaling for the Aurora Replica to have the appropriate number of replicas for the desired workload.
C. Create additional readers to cater to the different scenarios.
D. Use custom endpoints to satisfy the different workloads.
Answer: D
Explanation:
https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-c You can now create custom endpoints for
Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster. For
example, you may provision a set of Aurora Replicas to use an instance type with higher memory capacity in order to run an analytics workload. A custom endpoint
can then help you route the analytics workload to these
appropriately-configured instances, while keeping other instances in your cluster isolated from this workload. As you add or remove instances from the custom
endpoint to match your workload, the endpoint helps spread the load around.

NEW QUESTION 45
A financial services company is developing a shared data service that supports different applications from throughout the company. A Database Specialist
designed a solution to leverage Amazon ElastiCache for Redis with cluster mode enabled to enhance performance and scalability. The cluster is configured to
listen on port 6379.
Which combination of steps should the Database Specialist take to secure the cache data and protect it from unauthorized access? (Choose three.)
A. Enable in-transit and at-rest encryption on the ElastiCache cluster.
B. Ensure that Amazon CloudWatch metrics are configured in the ElastiCache cluster.
C. Ensure the security group for the ElastiCache cluster allows all inbound traffic from itself and inbound traffic on TCP port 6379 from trusted clients only.
D. Create an IAM policy to allow the application service roles to access all ElastiCache API actions.
E. Ensure the security group for the ElastiCache clients authorize inbound TCP port 6379 and port 22 traffic from the trusted ElastiCache cluster’s security group.
F. Ensure the cluster is created with the auth-token parameter and that the parameter is used in all subsequent commands.
Answer: ACF
Explanation:
https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html

NEW QUESTION 48
A significant automotive manufacturer is switching a mission-critical finance application's database to Amazon DynamoDB. According to the company's risk and
compliance policy, any update to the database must be documented as a log entry for auditing purposes. Each minute, the system anticipates about 500,000 log
entries. Log entries should be kept in Apache Parquet files in batches of at least 100,000 records per file.
How could a database professional approach these needs while using DynamoDB?
A. Enable Amazon DynamoDB Streams on the tabl
B. Create an AWS Lambda function triggered by the strea
C. Write the log entries to an Amazon S3 object.
D. Create a backup plan in AWS Backup to back up the DynamoDB table once a da
E. Create an AWS Lambda function that restores the backup in another table and compares both tables for change
F. Generate the log entries and write them to an Amazon S3 object.
G. Enable AWS CloudTrail logs on the tabl
H. Create an AWS Lambda function that reads the log files once an hour and filters DynamoDB API action
I. Write the filtered log files to Amazon S3.
J. Enable Amazon DynamoDB Streams on the tabl
K. Create an AWS Lambda function triggered by the strea
L. Write the log entries to an Amazon Kinesis Data Firehose delivery stream with buffering and Amazon S3 as the destination.
Answer: D

NEW QUESTION 51
A Database Specialist migrated an existing production MySQL database from on-premises to an Amazon RDS for MySQL DB instance. However, after the
migration, the database needed to be encrypted at rest using AWS KMS. Due to the size of the database, reloading, the data into an encrypted database would be
too time- consuming, so it is not an option.
How should the Database Specialist satisfy this new requirement?
A. Create a snapshot of the unencrypted RDS DB instanc
B. Create an encrypted copy of the unencrypted snapsho
C. Restore the encrypted snapshot copy.
D. Modify the RDS DB instanc
E. Enable the AWS KMS encryption option that leverages the AWS CLI.
F. Restore an unencrypted snapshot into a MySQL RDS DB instance that is encrypted.
G. Create an encrypted read replica of the RDS DB instanc
H. Promote it the master.
Answer: A
Explanation:
"However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can
create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and
thus you have an encrypted copy of your original DB instance. For more information, see Copying a Snapshot."
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html

NEW QUESTION 53
A database professional maintains a fleet of Amazon RDS database instances that are configured to utilize the default database parameter group. A database
expert must connect a custom parameter group with certain database instances.
When will the instances be allocated to this new parameter group once the database specialist performs this change?
A. Instantaneously after the change is made to the parameter group
B. In the next scheduled maintenance window of the DB instances
C. After the DB instances are manually rebooted
D. Within 24 hours after the change is made to the parameter group
Answer: C
Explanation:
When you associate a new DB parameter group with a DB instance, the modified static and dynamic parameters are applied only after the DB instance is
rebooted.

NEW QUESTION 57
A company needs to migrate Oracle Database Standard Edition running on an Amazon EC2 instance to an Amazon RDS for Oracle DB instance with Multi-AZ.
The database supports an ecommerce website that runs continuously. The company can only provide a maintenance window of up to 5 minutes.
Which solution will meet these requirements?
A. Configure Oracle Real Application Clusters (RAC) on the EC2 instance and the RDS DB instance.Update the connection string to point to the RAC cluste
B. Once the EC2 instance and RDS DB instance are in sync, fail over from Amazon EC2 to Amazon RDS.

C. Export the Oracle database from the EC2 instance using Oracle Data Pump and perform an import into Amazon RD
D. Stop the application for the entire proces
E. When the import is complete, change thedatabase connection string and then restart the application.
F. Configure AWS DMS with the EC2 instance as the source and the RDS DB instance as the destination.Stop the application when the replication is in sync,
change the database connection string, and then restart the application.
G. Configure AWS DataSync with the EC2 instance as the source and the RDS DB instance as the destinatio
H. Stop the application when the replication is in sync, change the database connection string, and then restart the application.
Answer: C

NEW QUESTION 58
A financial institution uses AWS to host its online application. Amazon RDS for MySQL is used to host the application's database, which includes automatic
backups.
The program has corrupted the database logically, resulting in the application being unresponsive. The exact moment the corruption occurred has been
determined, and it occurred within the backup retention period.
How should a database professional restore a database to its previous state prior to corruption?
A. Use the point-in-time restore capability to restore the DB instance to the specified tim
B. No changes to the application connection string are required.
C. Use the point-in-time restore capability to restore the DB instance to the specified tim
D. Change the application connection string to the new, restored DB instance.
E. Restore using the latest automated backu
F. Change the application connection string to the new, restored DB instance.
G. Restore using the appropriate automated backu
H. No changes to the application connection string are required.
Answer: B
Explanation:
When you perform a restore operation to a point in time or from a DB Snapshot, a new DB Instance is created with a new endpoint (the old DB Instance can be
deleted if so desired). This is done to enable you to create multiple DB Instances from a specific DB Snapshot or point in time."

NEW QUESTION 61
A company has two separate AWS accounts: one for the business unit and another for corporate analytics. The company wants to replicate the business unit data
stored in Amazon RDS for MySQL in us-east-1 to its corporate analytics Amazon Redshift environment in us-west-1. The company wants to use AWS DMS with
Amazon RDS as the source endpoint and Amazon Redshift as the target endpoint.
Which action will allow AVS DMS to perform the replication?
A. Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.
B. Configure the AWS DMS replication instance in the same account as Amazon Redshift and in the same Region as Amazon RDS.
C. Configure the AWS DMS replication instance in its own account and in the same Region as Amazon Redshift.
D. Configure the AWS DMS replication instance in the same account and Region as Amazon RDS.
Answer: A
Explanation:
https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html

NEW QUESTION 65
A company is migrating its on-premises database workloads to the AWS Cloud. A database specialist performing the move has chosen AWS DMS to migrate an
Oracle database with a large table to Amazon RDS. The database specialist notices that AWS DMS is taking significant time to migrate the data.
Which actions would improve the data migration speed? (Choose three.)
A. Create multiple AWS DMS tasks to migrate the large table.
B. Configure the AWS DMS replication instance with Multi-AZ.
C. Increase the capacity of the AWS DMS replication server.
D. Establish an AWS Direct Connect connection between the on-premises data center and AWS.
E. Enable an Amazon RDS Multi-AZ configuration.
F. Enable full large binary object (LOB) mode to migrate all LOB data for all large tables.
Answer: CDE

NEW QUESTION 69
An Amazon RDS EBS-optimized instance with Provisioned IOPS (PIOPS) storage is using less than half of its allocated IOPS over the course of several hours
under constant load. The RDS instance exhibits multi-second read and write latency, and uses all of its maximum bandwidth for read throughput, yet the instance
uses less than half of its CPU and RAM resources.
What should a Database Specialist do in this situation to increase performance and return latency to sub- second levels?
A. Increase the size of the DB instance storage
B. Change the underlying EBS storage type to General Purpose SSD (gp2)
C. Disable EBS optimization on the DB instance
D. Change the DB instance to an instance class with a higher maximum bandwidth
Answer: D
Explanation:
https://docs.amazonaws.cn/en_us/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html

NEW QUESTION 72
A company has migrated a single MySQL database to Amazon Aurora. The production data is hosted in a DB cluster in VPC_PROD, and 12 testing environments
are hosted in VPC_TEST using the same AWS account. Testing results in minimal changes to the test data. The Development team wants each environment
refreshed nightly so each test database contains fresh production data every day.
Which migration approach will be the fastest and most cost-effective to implement?
A. Run the master in Amazon Aurora MySQ
B. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly.
C. Run the master in Amazon Aurora MySQ
D. Take a nightly snapshot, and restore it into 12 databases in VPC_TEST using Aurora Serverless.
E. Run the master in Amazon Aurora MySQ
F. Create 12 Aurora Replicas in VPC_TEST, and script the replicas to be deleted and re-created nightly.
G. Run the master in Amazon Aurora MySQL using Aurora Serverles
H. Create 12 clones in VPC_TEST, and script the clones to be deleted and re-created nightly.
Answer: A

NEW QUESTION 73
A financial company has allocated an Amazon RDS MariaDB DB instance with large storage capacity to
accommodate migration efforts. Post-migration, the company purged unwanted data from the instance. The company now want to downsize storage to save
money. The solution must have the least impact on production and near-zero downtime.
Which solution would meet these requirements?
A. Create a snapshot of the old databases and restore the snapshot with the required storage
B. Create a new RDS DB instance with the required storage and move the databases from the old instances to the new instance using AWS DMS
C. Create a new database using native backup and restore
D. Create a new read replica and make it the primary by terminating the existing primary
Answer: B
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/rds-db-storage-size/ Use AWS Database Migration Service (AWS DMS) for minimal downtime.

NEW QUESTION 75
A Database Specialist modified an existing parameter group currently associated with a production Amazon RDS for SQL Server Multi-AZ DB instance. The
change is associated with a static parameter type, which controls the number of user connections allowed on the most critical RDS SQL Server DB instance for the
company. This change has been approved for a specific maintenance window to help minimize the impact on users.
How should the Database Specialist apply the parameter group change for the DB instance?
A. Select the option to apply the change immediately
B. Allow the preconfigured RDS maintenance window for the given DB instance to control when the change is applied
C. Apply the change manually by rebooting the DB instance during the approved maintenance window
D. Reboot the secondary Multi-AZ DB instance
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html#USER_W

NEW QUESTION 77
A software development company is using Amazon Aurora MySQL DB clusters for several use cases, including development and reporting. These use cases
place unpredictable and varying demands on the Aurora DB clusters, and can cause momentary spikes in latency. System users run ad-hoc queries sporadically
throughout the week. Cost is a primary concern for the company, and a solution that does not require significant rework is needed.
Which solution meets these requirements?
A. Create new Aurora Serverless DB clusters for development and reporting, then migrate to these new DB clusters.
B. Upgrade one of the DB clusters to a larger size, and consolidate development and reporting activities on this larger DB cluster.
C. Use existing DB clusters and stop/start the databases on a routine basis using scheduling tools.
D. Change the DB clusters to the burstable instance family.
Answer: A
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html

NEW QUESTION 78
A Database Specialist is creating Amazon DynamoDB tables, Amazon CloudWatch alarms, and associated infrastructure for an Application team using a
development AWS account. The team wants a deployment method that will standardize the core solution components while managing environment-specific
settings separately, and wants to minimize rework due to configuration errors.
Which process should the Database Specialist recommend to meet these requirements?
A. Organize common and environmental-specific parameters hierarchically in the AWS Systems Manager Parameter Store, then reference the parameters
dynamically from an AWS CloudFormation templat
B. Deploy the CloudFormation stack using the environment name as a parameter.
C. Create a parameterized AWS CloudFormation template that builds the required object
D. Keep separate environment parameter files in separate Amazon S3 bucket
E. Provide an AWS CLI command that deploys the CloudFormation stack directly referencing the appropriate parameter bucket.
F. Create a parameterized AWS CloudFormation template that builds the required object
G. Import the template into the CloudFormation interface in the AWS Management Consol
H. Make the required changes to the parameters and deploy the CloudFormation stack.
I. Create an AWS Lambda function that builds the required objects using an AWS SD
J. Set the required parameter values in a test event in the Lambda console for each environment that the Application team can modify, as neede
K. Deploy the infrastructure by triggering the test event in the console.
Answer: A
Explanation:
https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/

NEW QUESTION 83
A company is going through a security audit. The audit team has identified cleartext master user password in the AWS CloudFormation templates for Amazon RDS
for MySQL DB instances. The audit team has flagged this as a security risk to the database team.
What should a database specialist do to mitigate this risk?
A. Change all the databases to use AWS IAM for authentication and remove all the cleartext passwords in CloudFormation templates.
B. Use an AWS Secrets Manager resource to generate a random password and reference the secret in the CloudFormation template.
C. Remove the passwords from the CloudFormation templates so Amazon RDS prompts for the password when the database is being created.
D. Remove the passwords from the CloudFormation template and store them in a separate fil
E. Replace the passwords by running CloudFormation using a sed command.
Answer: B
Explanation:
https://aws.amazon.com/blogs/infrastructure-and-automation/securing-passwords-in-aws-quick-starts-using-aws

NEW QUESTION 87
A Database Specialist is migrating a 2 TB Amazon RDS for Oracle DB instance to an RDS for PostgreSQL DB instance using AWS DMS. The source RDS Oracle
DB instance is in a VPC in the us-east-1 Region. The target RDS for PostgreSQL DB instance is in a VPC in the use-west-2 Region.
Where should the AWS DMS replication instance be placed for the MOST optimal performance?
A. In the same Region and VPC of the source DB instance
B. In the same Region and VPC as the target DB instance
C. In the same VPC and Availability Zone as the target DB instance
D. In the same VPC and Availability Zone as the source DB instance
Answer: C
Explanation:
https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.VPC.html#CHAP_ReplicationIn In fact, all the configurations list on above url
prefer the replication instance putting into target vpc region / subnet / az.
https://docs.aws.amazon.com/dms/latest/sbs/CHAP_SQLServer2Aurora.Steps.CreateReplicationInstance.html

NEW QUESTION 89
A database specialist was alerted that a production Amazon RDS MariaDB instance with 100 GB of storage was out of space. In response, the database specialist
modified the DB instance and added 50 GB of storage capacity. Three hours later, a new alert is generated due to a lack of free space on the same DB instance.
The database specialist decides to modify the instance immediately to increase its storage capacity by 20 GB.
What will happen when the modification is submitted?
A. The request will fail because this storage capacity is too large.
B. The request will succeed only if the primary instance is in active status.
C. The request will succeed only if CPU utilization is less than 10%.
D. The request will fail as the most recent modification was too soon.
Answer: D
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html

NEW QUESTION 94
A small startup firm wishes to move a 4 TB MySQL database from on-premises to AWS through an Amazon RDS for MySQL DB instance.
Which migration approach would result in the LEAST amount of downtime?
A. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente
B. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucke
C. Import the snapshot into the DB instance utilizing the MySQL utilities running on an Amazon EC2 instanc
D. Immediately point the application to the DB instance.
E. Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data cente
F. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve
G. Copy the snapshot into the EC2 instance and restore it into the EC2 MySQL instanc
H. Use AWS DMS to migrate data into a new RDS for MySQL DB instanc
I. Point the application to the DB instance.
J. Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data cente
K. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve
L. Copy the snapshot into an Amazon S3 bucket and import the snapshot into a new RDS for MySQL DB instance using the MySQL utilities running on an EC2
instanc
M. Point the application to the DB instance.
N. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente
O. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucke
P. Import the snapshot into the DB instance using the MySQL utilities running on an Amazon EC2 instanc
Q. Establish replication into the new DB instance using MySQL replicatio
R. Stop application access to the on-premises MySQL server and let the remaining transactions replicate ove
S. Point the application to the DB instance.
Answer: D
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.External.Repl.html

NEW QUESTION 97
A database specialist needs to configure an Amazon RDS for MySQL DB instance to close non-interactive connections that are inactive after 900 seconds.
What should the database specialist do to accomplish this task?
A. Create a custom DB parameter group and set the wait_timeout parameter value to 900. Associate the DB instance with the custom parameter group.
B. Connect to the MySQL database and run the SET SESSION wait_timeout=900 command.
C. Edit the my.cnf file and set the wait_timeout parameter value to 900. Restart the DB instance.
D. Modify the default DB parameter group and set the wait_timeout parameter value to 900.
Answer: A
Explanation:
https://aws.amazon.com/fr/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql- "You can set parameters globally using a
parameter group. Alternatively, you can set them for a particular session using the SET command."
https://aws.amazon.com/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql-pa

NEW QUESTION 100
An IT consulting company wants to reduce costs when operating its development environment databases. The company’s workflow creates multiple Amazon
Aurora MySQL DB clusters for each development group. The Aurora DB clusters are only used for 8 hours a day. The DB clusters can then be deleted at the end
of the development cycle, which lasts 2 weeks.
Which of the following provides the MOST cost-effective solution?
A. Use AWS CloudFormation template
B. Deploy a stack with the DB cluster for each development group.Delete the stack at the end of the development cycle.
C. Use the Aurora DB cloning featur
D. Deploy a single development and test Aurora DB instance, and create clone instances for the development group
E. Delete the clones at the end of the development cycle.
F. Use Aurora Replica
G. From the master automatic pause compute capacity option, create replicas for each development group, and promote each replica to maste
H. Delete the replicas at the end of the development cycle.
I. Use Aurora Serverles
J. Restore current Aurora snapshot and deploy to a serverless cluster for each development grou
K. Enable the option to pause the compute capacity on the cluster and set an appropriate timeout.
Answer: B
Explanation:
Aurora Serverless is not compatible to all Aurora provisioned engine version. However, you can do clone with most engine version. Meanwhile, I also consider the
performance while restoring snapshot to Aurora Serverless.
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html#aurora
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.us

NEW QUESTION 103
A large ecommerce company uses Amazon DynamoDB to handle the transactions on its web portal. Traffic patterns throughout the year are usually stable;
however, a large event is planned. The company knows that traffic will increase by up to 10 times the normal load over the 3-day event. When sale prices are
published during the event, traffic will spike rapidly.
How should a Database Specialist ensure DynamoDB can handle the increased traffic?
A. Ensure the table is always provisioned to meet peak needs
B. Allow burst capacity to handle the additional load
C. Set an AWS Application Auto Scaling policy for the table to handle the increase in traffic
D. Preprovision additional capacity for the known peaks and then reduce the capacity after the event
Answer: D
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition "DynamoDB provides some flexibility in your perpartition throughput provisioning by providing burst
capacity. Whenever you're not fully using a partition's throughput, DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle
usage spikes. DynamoDB currently retains up to 5 minutes (300 seconds) of unused read and write capacity. During an occasional burst of read or write activity,
these extra capacity units can be consumed quickly—even faster than the per-second provisioned throughput capacity that you've defined for your table.
DynamoDB can also consume burst capacity for background maintenance and other tasks without prior notice. Note that these burst capacity details might change
in the future."

NEW QUESTION 105
A company is using Amazon Aurora PostgreSQL for the backend of its application. The system users are complaining that the responses are slow. A database
specialist has determined that the queries to Aurora take longer during peak times. With the Amazon RDS Performance Insights dashboard, the load in the chart
for average active sessions is often above the line that denotes maximum CPU usage and the wait state shows that most wait events are IO:XactSync.
What should the company do to resolve these performance issues?
A. Add an Aurora Replica to scale the read traffic.
B. Scale up the DB instance class.
C. Modify applications to commit transactions in batches.
D. Modify applications to avoid conflicts by taking locks.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html https://blog.dbi-services.com/aws-aurora-xactsync-batchcommit/

NEW QUESTION 107
On AWS, a business is developing a web application. The application needs that the database supports concurrent read and write activities in several AWS
Regions. Additionally, the database must communicate data changes across Regions as they occur. The application must be highly available and have a latency
of less than a few hundred milliseconds.
Which solution satisfies these criteria?
A. Amazon DynamoDB global tables
B. Amazon DynamoDB streams with AWS Lambda to replicate the data
C. An Amazon ElastiCache for Redis cluster with cluster mode enabled and multiple shards
D. An Amazon Aurora global database
Answer: A
Explanation:
Aurora Global Databases provides a writer and a reader endpoints in the primary region but only a reader endpoints in other region. Although strongly consistent, it
does not fulfill the requirements that "there are plenty of read / write activities" in all regions.

NEW QUESTION 110
An electric utility company wants to store power plant sensor data in an Amazon DynamoDB table. The utility company has over 100 power plants and each power
plant has over 200 sensors that send data every 2 seconds. The sensor data includes time with milliseconds precision, a value, and a fault attribute if the sensor is
malfunctioning. Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. A database
specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant.
Which schema should the database specialist use when creating the DynamoDB table to achieve the fastest query time when looking for faulty sensors?
A. Use the plant identifier as the partition key and the measurement time as the sort ke
B. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.
C. Create a composite of the plant identifier and sensor identifier as the partition ke
D. Use the measurement time as the sort ke
E. Create a local secondary index (LSI) on the fault attribute.
F. Create a composite of the plant identifier and sensor identifier as the partition ke
G. Use the measurement time as the sort ke
H. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.
I. Use the plant identifier as the partition key and the sensor identifier as the sort ke
J. Create a local secondary index (LSI) on the fault attribute.
Answer: D
Explanation:
Plant id as partition key and Sensor id as a sort key. Fault can be identified quickly using the local secondary index and associated plant and sensor can be
identified easily.

NEW QUESTION 115
A database specialist is constructing an AWS CloudFormation stack using AWS CloudFormation. The database expert wishes to avoid the stack's Amazon RDS
ProductionDatabase resource being accidentally deleted.
Which solution will satisfy this criterion?
A. Create a stack policy to prevent update
B. Include €Effect€ : €ProductionDatabase€ and €Resource€€Deny€ in the policy.
C. Create an AWS CloudFormation stack in XML forma
D. Set xAttribute as false.
E. Create an RDS DB instance without the DeletionPolicy attribut
F. Disable termination protection.
G. Create a stack policy to prevent update
H. Include Effect, Deny, and Resource :ProductionDatabase in the policy.
Answer: D
Explanation:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html "When you set a stack policy, all resources are protected by
default. To allow updates on all resources, we add an Allow statement that allows all actions on all resources. Although the Allow statement specifies all resources,
the explicit Deny statement overrides it for the resource with the ProductionDatabase logical ID. This Deny statement prevents all update actions, such as
replacement or deletion, on the ProductionDatabase resource."

NEW QUESTION 119
A Database Specialist is migrating an on-premises Microsoft SQL Server application database to Amazon RDS for PostgreSQL using AWS DMS. The application
requires minimal downtime when the RDS DB instance goes live.
What change should the Database Specialist make to enable the migration?
A. Configure the on-premises application database to act as a source for an AWS DMS full load with ongoing change data capture (CDC)
B. Configure the AWS DMS replication instance to allow both full load and ongoing change data capture (CDC)
C. Configure the AWS DMS task to generate full logs to allow for ongoing change data capture (CDC)
D. Configure the AWS DMS connections to allow two-way communication to allow for ongoing change data capture (CDC)
Answer: A
Explanation:
"requires minimal downtime when the RDS DB instance goes live" in order to do CDC: "you must first ensure that ARCHIVELOG MODE is on to provide
information to LogMiner. AWS DMS uses LogMiner to read information from the archive logs so that AWS DMS can capture changes"
https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle2postgresql.steps.configureoracle.html "If you want to capture and apply changes (CDC), then you also
need the following privileges."

NEW QUESTION 120
A company is going to use an Amazon Aurora PostgreSQL DB cluster for an application backend. The DB cluster contains some tables with sensitive data. A
Database Specialist needs to control the access privileges at the table level.
How can the Database Specialist meet these requirements?
A. Use AWS IAM database authentication and restrict access to the tables using an IAM policy.
B. Configure the rules in a NACL to restrict outbound traffic from the Aurora DB cluster.
C. Execute GRANT and REVOKE commands that restrict access to the tables containing sensitive data.
D. Define access privileges to the tables containing sensitive data in the pg_hba.conf file.
Answer: C

NEW QUESTION 124
A manufacturing company’s website uses an Amazon Aurora PostgreSQL DB cluster.
Which configurations will result in the LEAST application downtime during a failover? (Choose three.)
A. Use the provided read and write Aurora endpoints to establish a connection to the Aurora DB cluster.
B. Create an Amazon CloudWatch alert triggering a restore in another Availability Zone when the primary Aurora DB cluster is unreachable.
C. Edit and enable Aurora DB cluster cache management in parameter groups.
D. Set TCP keepalive parameters to a high value.
E. Set JDBC connection string timeout variables to a low value.
F. Set Java DNS caching timeouts to a high value.
Answer: ABC

NEW QUESTION 128
A retail company with its main office in New York and another office in Tokyo plans to build a database solution on AWS. The company’s main workload consists
of a mission-critical application that updates its application data in a data store. The team at the Tokyo office is building dashboards with complex analytical queries
using the application data. The dashboards will be used to make buying decisions, so they need to have access to the application data in less than 1 second.
Which solution meets these requirements?
A. Use an Amazon RDS DB instance deployed in the us-east-1 Region with a read replica instance in the ap- northeast-1 Regio
B. Create an Amazon ElastiCache cluster in the ap-northeast-1 Region to cache application data from the replica to generate the dashboards.
C. Use an Amazon DynamoDB global table in the us-east-1 Region with replication into the ap-northeast-1 Regio
D. Use Amazon QuickSight for displaying dashboard results.
E. Use an Amazon RDS for MySQL DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Regio
F. Have the dashboard application read from the read replica.
G. Use an Amazon Aurora global databas
H. Deploy the writer instance in the us-east-1 Region and the replica in the ap-northeast-1 Regio
I. Have the dashboard application read from the replicaap-northeast-1 Region.
Answer: D
Explanation:
https://aws.amazon.com/blogs/database/aurora-postgresql-disaster-recovery-solutions-using-amazon-aurora-glob

NEW QUESTION 133
A company has a web-based survey application that uses Amazon DynamoDB. During peak usage, when survey responses are being collected, a Database
Specialist sees the
ProvisionedThroughputExceededException error.
What can the Database Specialist do to resolve this error? (Choose two.)
A. Change the table to use Amazon DynamoDB Streams
B. Purchase DynamoDB reserved capacity in the affected Region
C. Increase the write capacity units for the specific table
D. Change the table capacity mode to on-demand
E. Change the table type to throughput optimized
Answer: CD
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/switching.capacitymode.html

NEW QUESTION 136
A small startup company is looking to migrate a 4 TB on-premises MySQL database to AWS using an Amazon RDS for MySQL DB instance.
Which strategy would allow for a successful migration with the LEAST amount of downtime?
A. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente
B. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucke
C. Import the snapshot into the DB instance utilizing the MySQL utilities running on an Amazon EC2 instanc
D. Immediately point the application to the DB instance.
E. Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data cente
F. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve
G. Copy the snapshot into the EC2 instance and restore it into the EC2 MySQL instanc
H. Use AWS DMS to migrate data into a new RDS for MySQL DB instanc
I. Point the application to the DB instance.
J. Deploy a new Amazon EC2 instance, install the MySQL software on the EC2 instance, and configure networking for access from the on-premises data cente
K. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve
L. Copy the snapshot into an Amazon S3 bucket and import the snapshot into a new RDS for MySQL DB instance using the MySQL utilities running on an EC2
instanc
M. Point the application to the DB instance.
N. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente
O. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server, and copy it to an Amazon S3 bucke
P. Import the snapshot into the DB instance using the MySQL utilities running on an Amazon EC2 instanc
Q. Establish replication into the new DB instance using MySQL replicatio
R. Stop application access to the on-premises MySQL server and let the remaining transactions replicate ove
S. Point the application to the DB instance.
Answer: B

NEW QUESTION 139
A company developed a new application that is deployed on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances use the security
group named sg-application-servers. The company needs a database to store the data from the application and decides to use an Amazon RDS for MySQL DB
instance. The DB instance is deployed in private DB subnet.
What is the MOST restrictive configuration for the DB instance security group?
A. Only allow incoming traffic from the sg-application-servers security group on port 3306.
B. Only allow incoming traffic from the sg-application-servers security group on port 443.
C. Only allow incoming traffic from the subnet of the application servers on port 3306.
D. Only allow incoming traffic from the subnet of the application servers on port 443.
Answer: A
Explanation:
most restrictive approach is to allow only incoming connections from SG of EC2 instance on port 3306

NEW QUESTION 141
To meet new data compliance requirements, a company needs to keep critical data durably stored and readily accessible for 7 years. Data that is more than 1 year
old is considered archival data and must automatically be moved out of the Amazon Aurora MySQL DB cluster every week. On average, around 10 GB of new
data is added to the database every month. A database specialist must choose the most operationally efficient solution to migrate the archival data to Amazon S3.
Which solution meets these requirements?
A. Create a custom script that exports archival data from the DB cluster to Amazon S3 using a SQL view, then deletes the archival data from the DB cluste
B. Launch an Amazon EC2 instance with a weekly cron job to execute the custom script.
C. Configure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement, then deletes
the archival data from the DB cluste
D. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events).
E. Configure two AWS Lambda functions: one that exports archival data from the DB cluster to Amazon S3 using the mysqldump utility, and another that deletes
the archival data from the DB cluste
F. Schedule both Lambda functions to run weekly using Amazon EventBridge (Amazon CloudWatch Events).
G. Use AWS Database Migration Service (AWS DMS) to continually export the archival data from the DB cluster to Amazon S3. Configure an AWS Data Pipeline
process to run weekly that executes a custom SQL script to delete the archival data from the DB cluster.
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.htm

NEW QUESTION 145
A large retail company recently migrated its three-tier ecommerce applications to AWS. The company’s backend database is hosted on Amazon Aurora
PostgreSQL. During peak times, users complain about longer page load times. A database specialist reviewed Amazon RDS Performance Insights and found a
spike in IO:XactSync wait events. The SQL attached to the wait events are all single INSERT statements.
How should this issue be resolved?
A. Modify the application to commit transactions in batches
B. Add a new Aurora Replica to the Aurora DB cluster.
C. Add an Amazon ElastiCache for Redis cluster and change the application to write through.
D. Change the Aurora DB cluster storage to Provisioned IOPS (PIOPS).
Answer: A
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html "This wait most often arises when there is a very high rate of commit activity on the system. You can
sometimes alleviate this wait by modifying applications to commit transactions in batches. "
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html

NEW QUESTION 150
A company is planning to close for several days. A Database Specialist needs to stop all applications along with the DB instances to ensure employees do not
have access to the systems during this time. All databases are running on Amazon RDS for MySQL.
The Database Specialist wrote and executed a script to stop all the DB instances. When reviewing the logs, the Database Specialist found that Amazon RDS DB
instances with read replicas did not stop.
How should the Database Specialist edit the script to fix this issue?
A. Stop the source instances before stopping their read replicas
B. Delete each read replica before stopping its corresponding source instance
C. Stop the read replicas before stopping their source instances
D. Use the AWS CLI to stop each read replica and source instance at the same time
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html
"The following are some limitations to stopping and starting a DB instance: You can't stop a DB instance that has a read replica, or that is a read replica." So if you
cant stop a db with a read replica, you have to delete the read replica first to then stop it???
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#U

NEW QUESTION 155
A Database Specialist needs to define a database migration strategy to migrate an on-premises Oracle database to an Amazon Aurora MySQL DB cluster. The
company requires near-zero downtime for the data migration. The solution must also be cost-effective.
Which approach should the Database Specialist take?
A. Dump all the tables from the Oracle database into an Amazon S3 bucket using datapump (expdp). Run data transformations in AWS Glu
B. Load the data from the S3 bucket to the Aurora DB cluster.
C. Order an AWS Snowball appliance and copy the Oracle backup to the Snowball applianc
D. Once the Snowball data is delivered to Amazon S3, create a new Aurora DB cluste
E. Enable the S3 integration to migrate the data directly from Amazon S3 to Amazon RDS.
F. Use the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migratio
G. Use AWS DMS to perform the full load and change data capture (CDC) tasks.
H. Use AWS Server Migration Service (AWS SMS) to import the Oracle virtual machine image as an Amazon EC2 instanc
I. Use the Oracle Logical Dump utility to migrate the Oracle data from Amazon EC2 to an Aurora DB cluster.
Answer: C
Explanation:
https://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/

NEW QUESTION 160
A business is launching a new Amazon RDS for SQL Server database instance. The organization wishes to allow auditing of the SQL Server database.
Which measures should a database professional perform in combination to achieve this requirement? (Select two.)
A. Create a service-linked role for Amazon RDS that grants permissions for Amazon RDS to store audit logs on Amazon S3.
B. Set up a parameter group to configure an IAM role and an Amazon S3 bucket for audit log storage.Associate the parameter group with the DB instance.
C. Disable Multi-AZ on the DB instance, and then enable auditin
D. Enable Multi-AZ after auditing is enabled.
E. Disable automated backup on the DB instance, and then enable auditin
F. Enable automated backup after auditing is enabled.
G. Set up an options group to configure an IAM role and an Amazon S3 bucket for audit log storage.Associate the options group with the DB instance.
Answer: AE
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.Audit.html
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/security_iam_service-with-iam.html

NEW QUESTION 163
A Database Specialist has migrated an on-premises Oracle database to Amazon Aurora PostgreSQL. The schema and the data have been migrated successfully.
The on-premises database server was also being used to run database maintenance cron jobs written in Python to perform tasks including data purging and
generating data exports. The logs for these jobs show that, most of the time, the jobs completed within 5 minutes, but a few jobs took up to 10 minutes to
complete. These maintenance jobs need to be set up for Aurora PostgreSQL.
How can the Database Specialist schedule these jobs so the setup requires minimal maintenance and provides high availability?
A. Create cron jobs on an Amazon EC2 instance to run the maintenance jobs following the required schedule.
B. Connect to the Aurora host and create cron jobs to run the maintenance jobs following the required schedule.
C. Create AWS Lambda functions to run the maintenance jobs and schedule them with Amazon CloudWatch Events.
D. Create the maintenance job using the Amazon CloudWatch job scheduling plugin.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.ht https://docs.aws.amazon.com/prescriptiveguidance/latest/patterns/schedule-jobs-for-amazon-rds-and-aurora-pos a job for data extraction or a job for data purging can easily be scheduled using cron. For these jobs, database credentials are typically either hard-coded or stored in a properties file. However, when you migrate to Amazon Relational Database Service
(Amazon RDS) or Amazon Aurora PostgreSQL, you lose the ability to log in to the host instance to schedule cron jobs. This pattern describes how to use AWS
Lambda and AWS Secrets Manager to schedule jobs for Amazon RDS and Aurora PostgreSQL databases after migration.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html

NEW QUESTION 165
A Database Specialist is creating a new Amazon Neptune DB cluster, and is attempting to load fata from Amazon S3 into the Neptune DB cluster using the
Neptune bulk loader API. The Database Specialist receives the following error:
“Unable to connect to s3 endpoint. Provided source = s3://mybucket/graphdata/ and region = us-east-1. Please verify your S3 configuration.”
Which combination of actions should the Database Specialist take to troubleshoot the problem? (Choose two.)
A. Check that Amazon S3 has an IAM role granting read access to Neptune
B. Check that an Amazon S3 VPC endpoint exists
C. Check that a Neptune VPC endpoint exists
D. Check that Amazon EC2 has an IAM role granting read access to Amazon S3
E. Check that Neptune has an IAM role granting read access to Amazon S3
Answer: BD

NEW QUESTION 169
A company has an application that uses an Amazon DynamoDB table to store user data. Every morning, a single-threaded process calls the DynamoDB API Scan
operation to scan the entire table and generate a critical start-of-day report for management. A successful marketing campaign recently doubled the number of
items in the table, and now the process takes too long to run and the report is not generated in time.
A database specialist needs to improve the performance of the process. The database specialist notes that, when the process is running, 15% of the table’s
provisioned read capacity units (RCUs) are being used.
What should the database specialist do?
A. Enable auto scaling for the DynamoDB table.
B. Use four threads and parallel DynamoDB API Scan operations.
C. Double the table’s provisioned RCUs.
D. Set the Limit and Offset parameters before every call to the API.
Answer: B
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan

NEW QUESTION 171
An online shopping company has a large inflow of shopping requests daily. As a result, there is a consistent load on the company’s Amazon RDS database. A
database specialist needs to ensure the database is up and running at all times. The database specialist wants an automatic notification system for issues that
may cause database downtime or for configuration changes made to the database.
What should the database specialist do to achieve this? (Choose two.)
A. Create an Amazon CloudWatch Events event to send a notification using Amazon SNS on every API call logged in AWS CloudTrail.
B. Subscribe to an RDS event subscription and configure it to use an Amazon SNS topic to send notifications.
C. Use Amazon SES to send notifications based on configured Amazon CloudWatch Events events.
D. Configure Amazon CloudWatch alarms on various metrics, such as FreeStorageSpace for the RDS instance.
E. Enable email notifications for AWS Trusted Advisor.
Answer: BD

NEW QUESTION 174
A business's production database is hosted on a single-node Amazon RDS for MySQL DB instance. The database instance is hosted in a United States AWS
Region.
A week before a significant sales event, a fresh database maintenance update is released. The maintenance update has been designated as necessary. The firm
want to minimize the database instance's downtime and requests that a database expert make the database instance highly accessible until the sales event
concludes.
Which solution will satisfy these criteria?
A. Defer the maintenance update until the sales event is over.
B. Create a read replica with the latest updat
C. Initiate a failover before the sales event.
D. Create a read replica with the latest updat
E. Transfer all read-only traffic to the read replica during the sales event.
F. Convert the DB instance into a Multi-AZ deploymen
G. Apply the maintenance update.
Answer: D
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/

NEW QUESTION 177
A Database Specialist must create a read replica to isolate read-only queries for an Amazon RDS for MySQL DB instance. Immediately after creating the read
replica, users that query it report slow response times. What could be causing these slow response times?
A. New volumes created from snapshots load lazily in the background
B. Long-running statements on the master
C. Insufficient resources on the master
D. Overload of a single replication thread by excessive writes on the master
Answer: A
Explanation:
snapshot is lazy loaded If the volume is accessed where the data is not loaded, the application accessing the volume encounters a higher latency than normal
while the data gets loaded
https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-p

NEW QUESTION 180
Amazon Neptune is being used by a corporation as the graph database for one of its products. During an ETL procedure, the company's data science team
produced enormous volumes of temporary data by unintentionally. The Neptune DB cluster extended its storage capacity automatically to handle the added data,
but the data science team erased the superfluous data.
What should a database professional do to prevent incurring extra expenditures for cluster volume space that is not being used?
A. Take a snapshot of the cluster volum
B. Restore the snapshot in another cluster with a smaller volume size.
C. Use the AWS CLI to turn on automatic resizing of the cluster volume.
D. Export the cluster data into a new Neptune DB cluster.
E. Add a Neptune read replica to the cluste
F. Promote this replica as a new primary DB instanc
G. Reset the storage space of the cluster.
Answer: C
Explanation:
The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph
and then reload it into a new DB cluster. Creating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster, because a
snapshot retains the original image of the cluster's underlying storage.

NEW QUESTION 182
A stock market analysis firm maintains two locations: one in the us-east-1 Region and another in the eu-west-2 Region. The business want to build an AWS
database solution capable of providing rapid and accurate updates.
Dashboards with advanced analytical queries are used to present data in the eu-west-2 office. Because the corporation will use these dashboards to make
purchasing choices, they must have less than a second to obtain application data.
Which solution satisfies these criteria and gives the MOST CURRENT dashboard?
A. Deploy an Amazon RDS DB instance in us-east-1 with a read replica instance in eu-west-2. Create an Amazon ElastiCache cluster in eu-west-2 to cache data
from the read replica to generate the dashboards.
B. Use an Amazon DynamoDB global table in us-east-1 with replication into eu-west-2. Use multi-active replication to ensure that updates are quickly propagated
to eu-west-2.
C. Use an Amazon Aurora global databas
D. Deploy the primary DB cluster in us-east-1. Deploy the secondary DB cluster in eu-west-2. Configure the dashboard application to read from the secondary
cluster.
E. Deploy an Amazon RDS for MySQL DB instance in us-east-1 with a read replica instance in eu-west-2.Configure the dashboard application to read from the
read replica.
Answer: C
Explanation:
Amazon Aurora global databases span multiple AWS Regions, enabling low latency global reads and providing fast recovery from the rare outage that might affect
an entire AWS Region. An Aurora global database has a primary DB cluster in one Region, and up to five secondary DB clusters in different Regions.
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html

NEW QUESTION 186
A database specialist must create nightly backups of an Amazon DynamoDB table in a mission-critical workload as part of a disaster recovery strategy.
Which backup methodology should the database specialist use to MINIMIZE management overhead?
A. Install the AWS CLI on an Amazon EC2 instanc
B. Write a CLI command that creates a backup of the DynamoDB tabl
C. Create a scheduled job or task that executes the command on a nightly basis.
D. Create an AWS Lambda function that creates a backup of the DynamoDB tabl
E. Create an Amazon CloudWatch Events rule that executes the Lambda function on a nightly basis.
F. Create a backup plan using AWS Backup, specify a backup frequency of every 24 hours, and give the plan a nightly backup window.
G. Configure DynamoDB backup and restore for an on-demand backup frequency of every 24 hours.
Answer: C
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CreateBackup.html#:~:text=If%20you%2
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html

NEW QUESTION 188
A company is looking to migrate a 1 TB Oracle database from on-premises to an Amazon Aurora PostgreSQL DB cluster. The company’s Database Specialist
discovered that the Oracle database is storing 100 GB of large binary objects (LOBs) across multiple tables. The Oracle database has a maximum LOB size of 500
MB with an average LOB size of 350 MB. The Database Specialist has chosen AWS DMS to migrate the data with the largest replication instances.
How should the Database Specialist optimize the database migration using AWS DMS?
A. Create a single task using full LOB mode with a LOB chunk size of 500 MB to migrate the data and LOBs together
B. Create two tasks: task1 with LOB tables using full LOB mode with a LOB chunk size of 500 MB and task2 without LOBs
C. Create two tasks: task1 with LOB tables using limited LOB mode with a maximum LOB size of 500 MB and task 2 without LOBs
D. Create a single task using limited LOB mode with a maximum LOB size of 500 MB to migrate data and LOBs together
Answer: C

NEW QUESTION 192
A worldwide digital advertising corporation collects browser information in order to provide targeted visitors with contextually relevant pictures, websites, and
connections. A single page load may create many events, each of which must be kept separately. A single event may have a maximum size of 200 KB and an
average size of 10 KB. Each page load requires a query of the user's browsing history in order to deliver suggestions for targeted advertising. The advertising
corporation anticipates daily page views of more than 1 billion from people in the United States, Europe, Hong Kong, and India. The information structure differs
according to the event. Additionally, browsing information must be written and read with a very low latency to guarantee that consumers have a positive viewing
experience.
Which database solution satisfies these criteria?
A. Amazon DocumentDB
B. Amazon RDS Multi-AZ deployment
C. Amazon DynamoDB global table
D. Amazon Aurora Global Database
Answer: C

NEW QUESTION 193
A company developed an AWS CloudFormation template used to create all new Amazon DynamoDB tables in its AWS account. The template configures
provisioned throughput capacity using hard-coded values. The company wants to change the template so that the tables it creates in the future have independently
configurable read and write capacity units assigned.
Which solution will enable this change?
A. Add values for the rcuCount and wcuCount parameters to the Mappings section of the template.Configure DynamoDB to provision throughput capacity using
the stack’s mappings.
B. Add values for two Number parameters, rcuCount and wcuCount, to the templat
C. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters.
D. Add values for the rcuCount and wcuCount parameters as outputs of the templat
E. Configure DynamoDB to provision throughput capacity using the stack outputs.
F. Add values for the rcuCount and wcuCount parameters to the Mappings section of the templat
G. Replace the hard-coded values with calls to the Ref intrinsic function, referencing the new parameters.
Answer: B
Explanation:
Input parameter and FindInMap You can use an input parameter with the Fn::FindInMap function to refer to a specific value in a map. For example, suppose you
have a list of regions and environment types that map to a specific AMI ID. You can select the AMI ID that your stack uses by using an input parameter
(EnvironmentType). To determine the region, use the AWS::Region pseudo parameter, which gets the AWS Region in which you create the stack.
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html

NEW QUESTION 196
A Database Specialist is constructing a new Amazon Neptune DB cluster and tries to load data from Amazon S3 using the Neptune bulk loader API. The Database
Specialist is confronted with the following error message:
€Unable to establish a connection to the s3 endpoint. The source URL is s3:/mybucket/graphdata/ and the region code is us-east-1. Kindly confirm your
Configuration S3.
Which of the following activities should the Database Specialist take to resolve the issue? (Select two.)
A. Check that Amazon S3 has an IAM role granting read access to Neptune
B. Check that an Amazon S3 VPC endpoint exists
C. Check that a Neptune VPC endpoint exists
D. Check that Amazon EC2 has an IAM role granting read access to Amazon S3
E. Check that Neptune has an IAM role granting read access to Amazon S3
Answer: BE
Explanation:
https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html
“An IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and
List permissions.” “An Amazon S3 VPC endpoint. For more information, see the Creating an Amazon S3 VPC Endpoint section.”

NEW QUESTION 199
A bank intends to utilize Amazon RDS to host a MySQL database instance. The database should be able to handle high-volume read requests with extremely few
repeated queries.
Which solution satisfies these criteria?
A. Create an Amazon ElastiCache cluste
B. Use a write-through strategy to populate the cache.
C. Create an Amazon ElastiCache cluste
D. Use a lazy loading strategy to populate the cache.
E. Change the DB instance to Multi-AZ with a standby instance in another AWS Region.
F. Create a read replica of the DB instanc
G. Use the read replica to distribute the read traffic.
Answer: D

NEW QUESTION 203
A company is using Amazon with Aurora Replicas for read-only workload scaling. A Database Specialist needs to split up two read-only applications so each
application always connects to a dedicated replica. The Database Specialist wants to implement load balancing and high availability for the read-only applications.
Which solution meets these requirements?
A. Use a specific instance endpoint for each replica and add the instance endpoint to each read-only application connection string.
B. Use reader endpoints for both the read-only workload applications.
C. Use a reader endpoint for one read-only application and use an instance endpoint for the other read-only application.
D. Use custom endpoints for the two read-only applications.
Answer: D
Explanation:
https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-c

NEW QUESTION 204
A business just transitioned from an on-premises Oracle database to Amazon Aurora PostgreSQL. Following the move, the organization observed that every day
around 3:00 PM, the application's response time is substantially slower. The firm has determined that the problem is with the database, not the application.
Which set of procedures should the Database Specialist do to locate the erroneous PostgreSQL query most efficiently?
A. Create an Amazon CloudWatch dashboard to show the number of connections, CPU usage, and disk space consumptio
B. Watch these dashboards during the next slow period.
C. Launch an Amazon EC2 instance, and install and configure an open-source PostgreSQL monitoring tool that will run reports based on the output error logs.
D. Modify the logging database parameter to log all the queries related to locking in the database and then check the logs after the next slow period for this
information.
E. Enable Amazon RDS Performance Insights on the PostgreSQL databas
F. Use the metrics to identify any queries that are related to spikes in the graph during the next slow period.
Answer: D
Explanation:
https://aws.amazon.com/blogs/database/optimizing-and-tuning-queries-in-amazon-rds-postgresql-based-on-nativ "AWS recently released a feature called Amazon
RDS Performance Insights, which provides an
easy-to-understand dashboard for detecting performance problems in terms of load." "AWS recently released a feature called Amazon RDS Performance Insights,
which provides an easy-to-understand dashboard for detecting performance problems in terms of load."

NEW QUESTION 208
A company wants to migrate its Microsoft SQL Server Enterprise Edition database instance from on-premises to AWS. A deep review is performed and the AWS
Schema Conversion Tool (AWS SCT) provides options for running this workload on Amazon RDS for SQL Server Enterprise Edition, Amazon RDS for SQL Server
Standard Edition, Amazon Aurora MySQL, and Amazon Aurora PostgreSQL. The company does not want to use its own SQL server license and does not want to
change from Microsoft SQL Server.
What is the MOST cost-effective and operationally efficient solution?
A. Run SQL Server Enterprise Edition on Amazon EC2.
B. Run SQL Server Standard Edition on Amazon RDS.
C. Run SQL Server Enterprise Edition on Amazon RDS.
D. Run Amazon Aurora MySQL leveraging SQL Server on Linux compatibility libraries.
Answer: B
Explanation:
This link seems to indicate that more information is required to determine if the Enterprise instance is a candidate for downgrading to Standard.
https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/determine-whether-your-microsoft-sql-server
https://calculator.aws/#/createCalculator/RDSSQLServer

NEW QUESTION 209
In one AWS account, a business runs a two-tier ecommerce application. An Amazon RDS for MySQL
Multi-AZ database instance serves as the application's backend. A developer removed the database instance in the production environment by accident. Although
the organization recovers the database, the incident results in hours of outage and financial loss.
Which combination of adjustments would reduce the likelihood that this error will occur again in the future? (Select three.)
A. Grant least privilege to groups, IAM users, and roles.
B. Allow all users to restore a database from a backup.
C. Enable deletion protection on existing production DB instances.
D. Use an ACL policy to restrict users from DB instance deletion.
E. Enable AWS CloudTrail logging and Enhanced Monitoring.
Answer: ACD

NEW QUESTION 213
An online retail company is planning a multi-day flash sale that must support processing of up to 5,000 orders per second. The number of orders and exact
schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before buying items. Outside of the sale, the
traffic volume is very low. The acceptable performance for read/write queries should be under 25 ms. Order items are about 2 KB in size and have a unique
identifier. The company requires the most cost-effective solution that will automatically scale and is highly available.
Which solution meets these requirements?
A. Amazon DynamoDB with on-demand capacity mode
B. Amazon Aurora with one writer node and an Aurora Replica with the parallel query feature enabled
C. Amazon DynamoDB with provisioned capacity mode with 5,000 write capacity units (WCUs) and 10,000 read capacity units (RCUs)
D. Amazon Aurora with one writer node and two cross-Region Aurora Replicas
Answer: A
Explanation:
The number of orders and exact schedule for the sale will vary each day. During the sale, approximately 10,000 concurrent users will look at the deals before
buying items. Outside of the sale, the traffic volume is very low ==> Setting provisioning DynamoDB fix read 5000/write 10000 with will waste the resource when
the traffic is low. It is not cost-effective.