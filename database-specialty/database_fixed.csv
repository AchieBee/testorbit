question,ans1,ans2,ans3,ans4,ans5,ans6,correct_ans,check_radio,topic,domain,explanation
,A. Set the max_connections parameter to 16 000 in the instance-level parameter group.,B. Modify the client connection timeout to 300 seconds.,C. Create an Amazon RDS Proxy database proxy and update client connections to point to the proxy endpoint.,D. Enable the query cache at the instance level.,,,3,radio,,,Amazon RDS Proxy allows applications to pool and share connections established with the database improving database efficiency and application scalability. With RDS Proxy  failover times for Aurora and RDS databases are reduced by up to 66% and database credentials  authentication  and access can be managed through integration with AWS Secrets Manager and AWS Identity and Access Management (IAM). https://aws.amazon.com/rds/proxy/
A corporation intends to migrate a 500-GB Oracle database to Amazon Aurora PostgreSQL utilizing the AWS Schema Conversion Tool (AWS SCT) and AWS Data Management Service (AWS DMS). The database does not have any stored procedures  but does contain several huge or partitioned tables. Because the program is vital to the company  it is preferable to migrate with little downtime. Which measures should a database professional perform in combination to expedite the transfer process? (Select three.),A. Use the AWS SCT data extraction agent to migrate the schema from Oracle to Aurora PostgreSQL.,B. For the large tables  change the setting for the maximum number of tables to load in parallel and perform a full load using AWS DMS.,C. For the large tables  create a table settings rule with a parallel load option in AWS DMS  then perform a full load using DMS.,D. Use AWS DMS to set up change data capture (CDC) for continuous replication until the cutover date.,E. Use AWS SCT to convert the schema from Oracle to Aurora PostgreSQL.,F. Use AWS DMS to convert the schema from Oracle to Aurora PostgreSQL and for continuous replication.,3,radio,,,
A financial services organization employs an Amazon Aurora PostgreSQL DB cluster to host an application on AWS. No log files detailing database administrator activity were discovered during a recent examination. A database professional must suggest a solution that enables access to the database and maintains activity logs. The solution should be simple to implement and have a negligible effect on performance. Which database specialist solution should be recommended?,A. Enable Aurora Database Activity Streams on the database in synchronous mod,B. Connect the Amazon Kinesis data stream to Kinesis Data Firehos,C. Set the Kinesis Data Firehose destination to an Amazon S3 bucket.,D. Create an AWS CloudTrail trail in the Region where the database run,E. Associate the database activity logs with the trail.,F. Enable Aurora Database Activity Streams on the database in asynchronous mod G. Connect the Amazon Kinesis data stream to Kinesis Data Firehos H. Set the Firehose destination to an Amazon S3 bucket. I. Allow connections to the DB cluster through a bastion host onl J. Restrict database access to the bastion host and application server K. Push the bastion host logs to Amazon CloudWatch Logs using the CloudWatch Logs agent.,3,radio,,,https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/DBActivityStreams.Overview.html
A user has a non-relational key-value database. The user is looking for a fully managed AWS service that will offload the administrative burdens of operating and scaling distributed databases. The solution must be cost- effective and able to handle unpredictable application traffic. What should a Database Specialist recommend for this user?,A. Create an Amazon DynamoDB table with provisioned capacity mode,B. Create an Amazon DocumentDB cluster,C. Create an Amazon DynamoDB table with on-demand capacity mode,D. Create an Amazon Aurora Serverless DB cluster,,,3,radio,,,
A company has an ecommerce web application with an Amazon RDS for MySQL DB instance. The marketing team has noticed some unexpected updates to the product and pricing information on the website  which is impacting sales targets. The marketing team wants a database specialist to audit future database activity to help identify how and when the changes are being made. What should the database specialist do to meet these requirements? (Choose two.),A. Create an RDS event subscription to the audit event type.,B. Enable auditing of CONNECT and QUERY_DML events.,C. SSH to the DB instance and review the database logs.,D. Publish the database logs to Amazon CloudWatch Logs.,E. Enable Enhanced Monitoring on the DB instance.,,2,radio,,,https://aws.amazon.com/blogs/database/configuring-an-audit-log-to-capture-database-activities-for-amazon-rds
Recently  a financial institution created a portfolio management service. The application's backend is powered by Amazon Aurora  which supports MySQL. The firm demands a response time of five minutes and a response time of five minutes. A database professional must create a disaster recovery system that is both efficient and has a low replication latency. How should the database professional tackle these requirements?,A. Configure AWS Database Migration Service (AWS DMS) and create a replica in a different AWS Region.,B. Configure an Amazon Aurora global database and add a different AWS Region.,C. Configure a binlog and create a replica in a different AWS Region.,D. Configure a cross-Region read replica.,,,2,radio,,,https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-disaster-recovery.ht https://aws.amazon.com/blogs/database/how-tochoose-the-best-disaster-recovery-option-for-your-amazon-auro https://aws.amazon.com/about-aws/whats-new/2019/11/aurora-supports-in-place-conversion-to-global-database/
A Database Specialist is performing a proof of concept with Amazon Aurora using a small instance to confirm a simple database behavior. When loading a large dataset and creating the index  the Database Specialist encounters the following error message from Aurora: ERROR: cloud not write block 7507718 of temporary file: No space left on device What is the cause of this error and what should the Database Specialist do to resolve this issue?,A. The scaling of Aurora storage cannot catch up with the data loadin,B. The Database Specialist needs to modify the workload to load the data slowly.,C. The scaling of Aurora storage cannot catch up with the data loadin,D. The Database Specialist needs to enable Aurora storage scaling.,E. The local storage used to store temporary tables is ful,F. The Database Specialist needs to scale up the instance. G. The local storage used to store temporary tables is ful H. The Database Specialist needs to enable localstorage scaling.,3,radio,,,
A financial company wants to store sensitive user data in an Amazon Aurora PostgreSQL DB cluster. The database will be accessed by multiple applications across the company. The company has mandated that all communications to the database be encrypted and the server identity must be validated. Any non-SSLbased connections should be disallowed access to the database. Which solution addresses these requirements?,A. Set the rds.force_ssl=0 parameter in DB parameter group,B. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=allow.,C. Set the rds.force_ssl=1 parameter in DB parameter group,D. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=disable.,E. Set the rds.force_ssl=0 parameter in DB parameter group,F. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-ca. G. Set the rds.force_ssl=1 parameter in DB parameter group H. Download and use the Amazon RDS certificate bundle and configure the PostgreSQL connection string with sslmode=verify-full.,4,radio,,,PostgreSQL: sslrootcert=rds-cert.pem sslmode=[verify-ca | verify-full]
A business that specializes in internet advertising is developing an application that will show adverts to its customers. The program stores data in an Amazon DynamoDB database. Additionally  the application caches its reads using a DynamoDB Accelerator (DAX) cluster. The majority of reads come via the GetItem and BatchGetItem queries. The application does not need consistency of readings. The application cache does not behave as intended after deployment. Specific extremely consistent queries to the DAX cluster are responding in several milliseconds rather than microseconds. How can the business optimize cache behavior in order to boost application performance?,A. Increase the size of the DAX cluster.,B. Configure DAX to be an item cache with no query cache,C. Use eventually consistent reads instead of strongly consistent reads.,D. Create a new DAX cluster with a higher TTL for the item cache.,,,3,radio,,,
A company has an AWS CloudFormation template written in JSON that is used to launch new Amazon RDS for MySQL DB instances. The security team has asked a database specialist to ensure that the master password is automatically rotated every 30 days for all new DB instances that are launched using the template. What is the MOST operationally efficient solution to meet these requirements?,A. Save the password in an Amazon S3 objec,B. Encrypt the S3 object with an AWS KMS ke,C. Set the KMS key to be rotated every 30 days by setting the EnableKeyRotation property to tru,D. Use a CloudFormation custom resource to read the S3 object to extract the password.,E. Create an AWS Lambda function to rotate the secre,F. Modify the CloudFormation template to add an AWS::SecretsManager::RotationSchedule resourc G. Configure the RotationLambdaARN value and  for the RotationRules property  set the AutomaticallyAfterDays parameter to 30. H. Modify the CloudFormation template to use the AWS KMS key as the database passwor I. Configure an Amazon EventBridge rule to invoke the KMS API to rotate the key every 30 days by setting the ScheduleExpression parameter to ***/30***. J. Integrate the Amazon RDS for MySQL DB instances with AWS IAM and centrally manage the master database user password.,2,radio,,,https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-secretsmanager-rotationsche
A company is using 5 TB Amazon RDS DB instances and needs to maintain 5 years of monthly database backups for compliance purposes. A Database Administrator must provide Auditors with data within 24 hours. Which solution will meet these requirements and is the MOST operationally efficient?,A. Create an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot.Move the snapshot to the company’s Amazon S3 bucket.,B. Create an AWS Lambda function to run on the first day of every month to take a manual RDS snapshot.,C. Create an RDS snapshot schedule from the AWS Management Console to take a snapshot every 30days.,D. Create an AWS Lambda function to run on the first day of every month to create an automated RDS snapshot.,,,1,radio,,,Unlike automated backups  manual snapshots aren't subject to the backup retention period. Snapshots don't expire. For very long-term backups of MariaDB MySQL  and PostgreSQL data  we recommend exporting snapshot data to Amazon S3. If the major version of your DB engine is no longer supported  you can't restore to that version from a snapshot.
A business need a data warehouse system that stores data consistently and in a highly organized fashion. The organization demands rapid response times for enduser inquiries including current-year data  and users must have access to the whole 15-year dataset when necessary. Additionally  this solution must be able to manage a variable volume of incoming inquiries. Costs associated with storing the 100 TB of data must be maintained to a minimum. Which solution satisfies these criteria?,A. Leverage an Amazon Redshift data warehouse solution using a dense storage instance type while keeping all the data on local Amazon Redshift storag,B. Provision enough instances to support high demand.,C. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat,D. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye,E. Provision enough instances to support high demand.,F. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat G. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye H. Enable Amazon Redshift Concurrency Scaling. I. Leverage an Amazon Redshift data warehouse solution using a dense storage instance to store the most recent dat J. Keep historical data on Amazon S3 and access it using the Amazon Redshift Spectrum laye K. Leverage Amazon Redshift elastic resize.,3,radio,,,"With the Concurrency Scaling feature, you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance.
When concurrency scaling is enabled, Amazon Redshift automatically adds additional cluster capacity when you need it to process an increase in concurrent read
queries. Write operations continue as normal on your main cluster. Users always see the most current data, whether the queries run on the main cluster or on a
concurrency scaling cluster. You're charged for concurrency scaling clusters only for the time they're in use. For more information about pricing, see Amazon
Redshift pricing. You manage which queries are sent to the concurrency scaling cluster by configuring WLM queues. When you enable concurrency scaling for a
A company wants to automate the creation of secure test databases with random credentials to be stored safely for later use. The credentials should have sufficient information about each test database to initiate a connection and perform automated credential rotations. The credentials should not be logged or stored anywhere in an unencrypted form. Which steps should a Database Specialist take to meet these requirements using an AWS CloudFormation template?,A. Create the database with the MasterUserName and MasterUserPassword properties set to the default value,B. Then  create the secret with the user name and password set to the same default value,C. Add aSecret Target Attachment resource with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the databas,D. Finally  update the secret’s password value with a randomly generated string set by the GenerateSecretString property.,E. Add a Mapping property from the database Amazon Resource Name (ARN) to the secret AR,F. Then  create the secret with a chosen user name and a randomly generated password set by the GenerateSecretString propert G. Add the database with the MasterUserName and MasterUserPassword properties set to the user name of the secret. H. Add a resource of type AWS::SecretsManager::Secret and specify the GenerateSecretString property.Then  define the database user name in the SecureStringTemplate templat I. Create a resource for the database and reference the secret string for the MasterUserName and MasterUserPassword propertie J. Then  add a resource of type AWS::SecretsManagerSecretTargetAttachment with the SecretId and TargetId properties set to the Amazon Resource Names (ARNs) of the secret and the database. K. Create the secret with a chosen user name and a randomly generated password set by the GenerateSecretString propert L. Add an SecretTargetAttachment resource with the SecretId property set to the Amazon Resource Name (ARN) of the secret and the TargetId property set to a parameter value matching the desired database AR M. Then  create a database with the MasterUserName and MasterUserPassword properties set to the previously created values in the secret.,3,radio,,,
A company is running its line of business application on AWS  which uses Amazon RDS for MySQL at the persistent data store. The company wants to minimize downtime when it migrates the database to Amazon Aurora. Which migration method should a Database Specialist use?,A. Take a snapshot of the RDS for MySQL DB instance and create a new Aurora DB cluster with the option to migrate snapshots.,B. Make a backup of the RDS for MySQL DB instance using the mysqldump utility  create a new Aurora DB cluster  and restore the backup.,C. Create an Aurora Replica from the RDS for MySQL DB instance and promote the Aurora DB cluster.,D. Create a clone of the RDS for MySQL DB instance and promote the Aurora DB cluster.,,,3,radio,,,https://aws.amazon.com/blogs/database/best-practices-for-migrating-rds-for-mysql-databases-to-amazon-aurora/ https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Migrating.html#Aurora
A company has a heterogeneous six-node production Amazon Aurora DB cluster that handles online transaction processing (OLTP) for the core business and OLAP reports for the human resources department. To match compute resources to the use case  the company has decided to have the reporting workload for the human resources department be directed to two small nodes in the Aurora DB cluster  while every other workload goes to four large nodes in the same DB cluster. Which option would ensure that the correct nodes are always available for the appropriate workload while meeting these requirements?,A. Use the writer endpoint for OLTP and the reader endpoint for the OLAP reporting workload.,B. Use automatic scaling for the Aurora Replica to have the appropriate number of replicas for the desired workload.,C. Create additional readers to cater to the different scenarios.,D. Use custom endpoints to satisfy the different workloads.,,,4,radio,,,https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-c You can now create custom endpoints for Amazon Aurora databases. This allows you to distribute and load balance workloads across different sets of database instances in your Aurora cluster. For example  you may provision a set of Aurora Replicas to use an instance type with higher memory capacity in order to run an analytics workload. A custom endpoint can then help you route the analytics workload to these appropriately-configured instances  while keeping other instances in your cluster isolated from this workload. As you add or remove instances from the custom endpoint to match your workload  the endpoint helps spread the load around.
A financial services company is developing a shared data service that supports different applications from throughout the company. A Database Specialist designed a solution to leverage Amazon ElastiCache for Redis with cluster mode enabled to enhance performance and scalability. The cluster is configured to listen on port 6379. Which combination of steps should the Database Specialist take to secure the cache data and protect it from unauthorized access? (Choose three.),A. Enable in-transit and at-rest encryption on the ElastiCache cluster.,B. Ensure that Amazon CloudWatch metrics are configured in the ElastiCache cluster.,C. Ensure the security group for the ElastiCache cluster allows all inbound traffic from itself and inbound traffic on TCP port 6379 from trusted clients only.,D. Create an IAM policy to allow the application service roles to access all ElastiCache API actions.,E. Ensure the security group for the ElastiCache clients authorize inbound TCP port 6379 and port 22 traffic from the trusted ElastiCache cluster’s security group.,F. Ensure the cluster is created with the auth-token parameter and that the parameter is used in all subsequent commands.,1,radio,,,https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html
A significant automotive manufacturer is switching a mission-critical finance application's database to Amazon DynamoDB. According to the company's risk and compliance policy  any update to the database must be documented as a log entry for auditing purposes. Each minute  the system anticipates about 500 000 log entries. Log entries should be kept in Apache Parquet files in batches of at least 100 000 records per file. How could a database professional approach these needs while using DynamoDB?,B. According to the company's risk and compliance policy  any update to the database must be documented as a log entry for auditing purposes. Each minute  the system anticipates about 500 000 log entries. Log entries should be kept in Apache Parquet files in batches of at least 100 000 records per file. How could a database professional approach these needs while using DynamoDB?,A. Enable Amazon DynamoDB Streams on the tabl,B. Create an AWS Lambda function triggered by the strea,C. Write the log entries to an Amazon S3 object.,D. Create a backup plan in AWS Backup to back up the DynamoDB table once a da,E. Create an AWS Lambda function that restores the backup in another table and compares both tables for change,F. Generate the log entries and write them to an Amazon S3 object. G. Enable AWS CloudTrail logs on the tabl H. Create an AWS Lambda function that reads the log files once an hour and filters DynamoDB API action I. Write the filtered log files to Amazon S3. J. Enable Amazon DynamoDB Streams on the tabl K. Create an AWS Lambda function triggered by the strea L. Write the log entries to an Amazon Kinesis Data Firehose delivery stream with buffering and Amazon S3 as the destination.,4,radio,,
A Database Specialist migrated an existing production MySQL database from on-premises to an Amazon RDS for MySQL DB instance. However  after the migration  the database needed to be encrypted at rest using AWS KMS. Due to the size of the database  reloading  the data into an encrypted database would be too time- consuming  so it is not an option. How should the Database Specialist satisfy this new requirement?,A. Create a snapshot of the unencrypted RDS DB instanc,B. Create an encrypted copy of the unencrypted snapsho,C. Restore the encrypted snapshot copy.,D. Modify the RDS DB instanc,E. Enable the AWS KMS encryption option that leverages the AWS CLI.,F. Restore an unencrypted snapshot into a MySQL RDS DB instance that is encrypted. G. Create an encrypted read replica of the RDS DB instanc H. Promote it the master.,1,radio,,,"However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can
create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. You can then restore a DB instance from the encrypted snapshot, and
thus you have an encrypted copy of your original DB instance. For more information, see Copying a Snapshot. https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html"
A database professional maintains a fleet of Amazon RDS database instances that are configured to utilize the default database parameter group. A database expert must connect a custom parameter group with certain database instances. When will the instances be allocated to this new parameter group once the database specialist performs this change?,A. Instantaneously after the change is made to the parameter group,B. In the next scheduled maintenance window of the DB instances,C. After the DB instances are manually rebooted,D. Within 24 hours after the change is made to the parameter group,,,3,radio,,,When you associate a new DB parameter group with a DB instance  the modified static and dynamic parameters are applied only after the DB instance is rebooted.
A company needs to migrate Oracle Database Standard Edition running on an Amazon EC2 instance to an Amazon RDS for Oracle DB instance with Multi-AZ. The database supports an ecommerce website that runs continuously. The company can only provide a maintenance window of up to 5 minutes. Which solution will meet these requirements?,A. Configure Oracle Real Application Clusters (RAC) on the EC2 instance and the RDS DB instance.Update the connection string to point to the RAC cluste,B. Once the EC2 instance and RDS DB instance are in sync  fail over from Amazon EC2 to Amazon RDS.,C. Export the Oracle database from the EC2 instance using Oracle Data Pump and perform an import into Amazon RD,D. Stop the application for the entire proces,E. When the import is complete  change thedatabase connection string and then restart the application.,F. Configure AWS DMS with the EC2 instance as the source and the RDS DB instance as the destination.Stop the application when the replication is in sync change the database connection string  and then restart the application. G. Configure AWS DataSync with the EC2 instance as the source and the RDS DB instance as the destinatio H. Stop the application when the replication is in sync  change the database connection string  and then restart the application.,3,radio,,,
A financial institution uses AWS to host its online application. Amazon RDS for MySQL is used to host the application's database  which includes automatic backups. The program has corrupted the database logically  resulting in the application being unresponsive. The exact moment the corruption occurred has been determined  and it occurred within the backup retention period. How should a database professional restore a database to its previous state prior to corruption?,A. Use the point-in-time restore capability to restore the DB instance to the specified tim,B. No changes to the application connection string are required.,C. Use the point-in-time restore capability to restore the DB instance to the specified tim,D. Change the application connection string to the new  restored DB instance.,E. Restore using the latest automated backu,F. Change the application connection string to the new  restored DB instance. G. Restore using the appropriate automated backu H. No changes to the application connection string are required.,2,radio,,,"When you perform a restore operation to a point in time or from a DB Snapshot  a new DB Instance is created with a new endpoint (the old DB Instance can be deleted if so desired). This is done to enable you to create multiple DB Instances from a specific DB Snapshot or point in time."""
A company has two separate AWS accounts: one for the business unit and another for corporate analytics. The company wants to replicate the business unit data stored in Amazon RDS for MySQL in us-east-1 to its corporate analytics Amazon Redshift environment in us-west-1. The company wants to use AWS DMS with Amazon RDS as the source endpoint and Amazon Redshift as the target endpoint. Which action will allow AVS DMS to perform the replication?,A. Configure the AWS DMS replication instance in the same account and Region as Amazon Redshift.,B. Configure the AWS DMS replication instance in the same account as Amazon Redshift and in the same Region as Amazon RDS.,C. Configure the AWS DMS replication instance in its own account and in the same Region as Amazon Redshift.,D. Configure the AWS DMS replication instance in the same account and Region as Amazon RDS.,,,1,radio,,,https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html
A company is migrating its on-premises database workloads to the AWS Cloud. A database specialist performing the move has chosen AWS DMS to migrate an Oracle database with a large table to Amazon RDS. The database specialist notices that AWS DMS is taking significant time to migrate the data. Which actions would improve the data migration speed? (Choose three.),A. Create multiple AWS DMS tasks to migrate the large table.,B. Configure the AWS DMS replication instance with Multi-AZ.,C. Increase the capacity of the AWS DMS replication server.,D. Establish an AWS Direct Connect connection between the on-premises data center and AWS.,E. Enable an Amazon RDS Multi-AZ configuration.,F. Enable full large binary object (LOB) mode to migrate all LOB data for all large tables.,3,radio,,,
A database specialist needs to configure an Amazon RDS for MySQL DB instance to close non-interactive connections that are inactive after 900 seconds. What should the database specialist do to accomplish this task?,A. Create a custom DB parameter group and set the wait_timeout parameter value to 900. Associate the DB instance with the custom parameter group.,B. Connect to the MySQL database and run the SET SESSION wait_timeout=900 command.,C. Edit the my.cnf file and set the wait_timeout parameter value to 900. Restart the DB instance.,D. Modify the default DB parameter group and set the wait_timeout parameter value to 900.,,,1,radio,,,"https://aws.amazon.com/fr/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql- ""You can set parameters globally using a parameter group. Alternatively  you can set them for a particular session using the SET command."" https://aws.amazon.com/blogs/database/best-practices-for-configuring-parameters-for-amazon-rds-for-mysql-pa"
An IT consulting company wants to reduce costs when operating its development environment databases. The company’s workflow creates multiple Amazon Aurora MySQL DB clusters for each development group. The Aurora DB clusters are only used for 8 hours a day. The DB clusters can then be deleted at the end of the development cycle  which lasts 2 weeks. Which of the following provides the MOST cost-effective solution?,A. Use AWS CloudFormation template,B. Deploy a stack with the DB cluster for each development group.Delete the stack at the end of the development cycle.,C. Use the Aurora DB cloning featur,D. Deploy a single development and test Aurora DB instance  and create clone instances for the development group,E. Delete the clones at the end of the development cycle.,F. Use Aurora Replica G. From the master automatic pause compute capacity option  create replicas for each development group  and promote each replica to maste H. Delete the replicas at the end of the development cycle. I. Use Aurora Serverles J. Restore current Aurora snapshot and deploy to a serverless cluster for each development grou K. Enable the option to pause the compute capacity on the cluster and set an appropriate timeout.,2,radio,,,Aurora Serverless is not compatible to all Aurora provisioned engine version. However  you can do clone with most engine version. Meanwhile  I also consider the performance while restoring snapshot to Aurora Serverless. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html#aurora https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.us
A large ecommerce company uses Amazon DynamoDB to handle the transactions on its web portal. Traffic patterns throughout the year are usually stable; however  a large event is planned. The company knows that traffic will increase by up to 10 times the normal load over the 3-day event. When sale prices are published during the event  traffic will spike rapidly. How should a Database Specialist ensure DynamoDB can handle the increased traffic?,A. Ensure the table is always provisioned to meet peak needs,B. Allow burst capacity to handle the additional load,C. Set an AWS Application Auto Scaling policy for the table to handle the increase in traffic,D. Preprovision additional capacity for the known peaks and then reduce the capacity after the event,,,4,radio,,,"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html#bp-partition ""DynamoDB provides some flexibility in your perpartition throughput provisioning by providing burst capacity. Whenever you're not fully using a partition's throughput  DynamoDB reserves a portion of that unused capacity for later bursts of throughput to handle usage spikes. DynamoDB currently retains up to 5 minutes (300 seconds) of unused read and write capacity. During an occasional burst of read or write activity these extra capacity units can be consumed quickly—even faster than the per-second provisioned throughput capacity that you've defined for your table. DynamoDB can also consume burst capacity for background maintenance and other tasks without prior notice. Note that these burst capacity details might change in the future."""
A company is using Amazon Aurora PostgreSQL for the backend of its application. The system users are complaining that the responses are slow. A database specialist has determined that the queries to Aurora take longer during peak times. With the Amazon RDS Performance Insights dashboard  the load in the chart for average active sessions is often above the line that denotes maximum CPU usage and the wait state shows that most wait events are IO:XactSync. What should the company do to resolve these performance issues?,A. Add an Aurora Replica to scale the read traffic.,B. Scale up the DB instance class.,C. Modify applications to commit transactions in batches.,D. Modify applications to avoid conflicts by taking locks.,,,3,radio,,,https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html https://blog.dbi-services.com/aws-aurora-xactsync-batchcommit/
On AWS  a business is developing a web application. The application needs that the database supports concurrent read and write activities in several AWS Regions. Additionally  the database must communicate data changes across Regions as they occur. The application must be highly available and have a latency of less than a few hundred milliseconds. Which solution satisfies these criteria?,A. Amazon DynamoDB global tables,B. Amazon DynamoDB streams with AWS Lambda to replicate the data,C. An Amazon ElastiCache for Redis cluster with cluster mode enabled and multiple shards,D. An Amazon Aurora global database,,,1,radio,,,"Aurora Global Databases provides a writer and a reader endpoints in the primary region but only a reader endpoints in other region. Although strongly consistent  it does not fulfill the requirements that ""there are plenty of read / write activities"" in all regions."
An electric utility company wants to store power plant sensor data in an Amazon DynamoDB table. The utility company has over 100 power plants and each power plant has over 200 sensors that send data every 2 seconds. The sensor data includes time with milliseconds precision  a value  and a fault attribute if the sensor is malfunctioning. Power plants are identified by a globally unique identifier. Sensors are identified by a unique identifier within each power plant. A database specialist needs to design the table to support an efficient method of finding all faulty sensors within a given power plant. Which schema should the database specialist use when creating the DynamoDB table to achieve the fastest query time when looking for faulty sensors?,A. Use the plant identifier as the partition key and the measurement time as the sort ke,B. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key.,C. Create a composite of the plant identifier and sensor identifier as the partition ke,D. Use the measurement time as the sort ke,E. Create a local secondary index (LSI) on the fault attribute.,F. Create a composite of the plant identifier and sensor identifier as the partition ke G. Use the measurement time as the sort ke H. Create a global secondary index (GSI) with the plant identifier as the partition key and the fault attribute as the sort key. I. Use the plant identifier as the partition key and the sensor identifier as the sort ke J. Create a local secondary index (LSI) on the fault attribute.,4,radio,,,Plant id as partition key and Sensor id as a sort key. Fault can be identified quickly using the local secondary index and associated plant and sensor can be identified easily.
A database specialist is constructing an AWS CloudFormation stack using AWS CloudFormation. The database expert wishes to avoid the stack's Amazon RDS ProductionDatabase resource being accidentally deleted. Which solution will satisfy this criterion?,A. Create a stack policy to prevent update,B. Include €Effect€ : €ProductionDatabase€ and €Resource€€Deny€ in the policy.,C. Create an AWS CloudFormation stack in XML forma,D. Set xAttribute as false.,E. Create an RDS DB instance without the DeletionPolicy attribut,F. Disable termination protection. G. Create a stack policy to prevent update H. Include Effect  Deny  and Resource :ProductionDatabase in the policy.,"D. This Deny statement prevents all update actions  such as replacement or deletion  on the ProductionDatabase resource.""",4,radio,,
A Database Specialist is migrating an on-premises Microsoft SQL Server application database to Amazon RDS for PostgreSQL using AWS DMS. The application requires minimal downtime when the RDS DB instance goes live. What change should the Database Specialist make to enable the migration?,A. Configure the on-premises application database to act as a source for an AWS DMS full load with ongoing change data capture (CDC),B. Configure the AWS DMS replication instance to allow both full load and ongoing change data capture (CDC),C. Configure the AWS DMS task to generate full logs to allow for ongoing change data capture (CDC),D. Configure the AWS DMS connections to allow two-way communication to allow for ongoing change data capture (CDC),,,1,radio,,,"requires minimal downtime when the RDS DB instance goes live in order to do CDC: ""you must first ensure that ARCHIVELOG MODE is on to provide information to LogMiner. AWS DMS uses LogMiner to read information from the archive logs so that AWS DMS can capture changes"" https://docs.aws.amazon.com/dms/latest/sbs/chap-oracle2postgresql.steps.configureoracle.html ""If you want to capture and apply changes (CDC)  then you also need the following privileges."""
A company is going to use an Amazon Aurora PostgreSQL DB cluster for an application backend. The DB cluster contains some tables with sensitive data. A Database Specialist needs to control the access privileges at the table level. How can the Database Specialist meet these requirements?,A. Use AWS IAM database authentication and restrict access to the tables using an IAM policy.,B. Configure the rules in a NACL to restrict outbound traffic from the Aurora DB cluster.,C. Execute GRANT and REVOKE commands that restrict access to the tables containing sensitive data.,D. Define access privileges to the tables containing sensitive data in the pg_hba.conf file.,,,3,radio,,,
A manufacturing company’s website uses an Amazon Aurora PostgreSQL DB cluster. Which configurations will result in the LEAST application downtime during a failover? (Choose three.),A. Use the provided read and write Aurora endpoints to establish a connection to the Aurora DB cluster.,B. Create an Amazon CloudWatch alert triggering a restore in another Availability Zone when the primary Aurora DB cluster is unreachable.,C. Edit and enable Aurora DB cluster cache management in parameter groups.,D. Set TCP keepalive parameters to a high value.,E. Set JDBC connection string timeout variables to a low value.,F. Set Java DNS caching timeouts to a high value.,1,radio,,,
A retail company with its main office in New York and another office in Tokyo plans to build a database solution on AWS. The company’s main workload consists of a mission-critical application that updates its application data in a data store. The team at the Tokyo office is building dashboards with complex analytical queries using the application data. The dashboards will be used to make buying decisions  so they need to have access to the application data in less than 1 second. Which solution meets these requirements?,A. Use an Amazon RDS DB instance deployed in the us-east-1 Region with a read replica instance in the ap- northeast-1 Regio,B. Create an Amazon ElastiCache cluster in the ap-northeast-1 Region to cache application data from the replica to generate the dashboards.,C. Use an Amazon DynamoDB global table in the us-east-1 Region with replication into the ap-northeast-1 Regio,D. Use Amazon QuickSight for displaying dashboard results.,E. Use an Amazon RDS for MySQL DB instance deployed in the us-east-1 Region with a read replica instance in the ap-northeast-1 Regio,F. Have the dashboard application read from the read replica. G. Use an Amazon Aurora global databas H. Deploy the writer instance in the us-east-1 Region and the replica in the ap-northeast-1 Regio I. Have the dashboard application read from the replicaap-northeast-1 Region.,4,radio,,,https://aws.amazon.com/blogs/database/aurora-postgresql-disaster-recovery-solutions-using-amazon-aurora-glob
A company has a web-based survey application that uses Amazon DynamoDB. During peak usage  when survey responses are being collected  a Database Specialist sees the ProvisionedThroughputExceededException error. What can the Database Specialist do to resolve this error? (Choose two.),B. During peak usage  when survey responses are being collected  a Database Specialist sees the ProvisionedThroughputExceededException error. What can the Database Specialist do to resolve this error? (Choose two.),A. Change the table to use Amazon DynamoDB Streams,B. Purchase DynamoDB reserved capacity in the affected Region,C. Increase the write capacity units for the specific table,D. Change the table capacity mode to on-demand,E. Change the table type to throughput optimized,3,radio,,,https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/switching.capacitymode.html
A small startup company is looking to migrate a 4 TB on-premises MySQL database to AWS using an Amazon RDS for MySQL DB instance. Which strategy would allow for a successful migration with the LEAST amount of downtime?,A. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente,B. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server  and copy it to an Amazon S3 bucke,C. Import the snapshot into the DB instance utilizing the MySQL utilities running on an Amazon EC2 instanc,D. Immediately point the application to the DB instance.,E. Deploy a new Amazon EC2 instance  install the MySQL software on the EC2 instance  and configure networking for access from the on-premises data cente,F. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve G. Copy the snapshot into the EC2 instance and restore it into the EC2 MySQL instanc H. Use AWS DMS to migrate data into a new RDS for MySQL DB instanc I. Point the application to the DB instance. J. Deploy a new Amazon EC2 instance  install the MySQL software on the EC2 instance  and configure networking for access from the on-premises data cente K. Use the mysqldump utility to create a snapshot of the on-premises MySQL serve L. Copy the snapshot into an Amazon S3 bucket and import the snapshot into a new RDS for MySQL DB instance using the MySQL utilities running on an EC2 instanc M. Point the application to the DB instance. N. Deploy a new RDS for MySQL DB instance and configure it for access from the on-premises data cente O. Use the mysqldump utility to create an initial snapshot from the on-premises MySQL server  and copy it to an Amazon S3 bucke P. Import the snapshot into the DB instance using the MySQL utilities running on an Amazon EC2 instanc Q. Establish replication into the new DB instance using MySQL replicatio R. Stop application access to the on-premises MySQL server and let the remaining transactions replicate ove S. Point the application to the DB instance.,2,radio,,,
A company developed a new application that is deployed on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances use the security group named sg-application-servers. The company needs a database to store the data from the application and decides to use an Amazon RDS for MySQL DB instance. The DB instance is deployed in private DB subnet. What is the MOST restrictive configuration for the DB instance security group?,A. Only allow incoming traffic from the sg-application-servers security group on port 3306.,B. Only allow incoming traffic from the sg-application-servers security group on port 443.,C. Only allow incoming traffic from the subnet of the application servers on port 3306.,D. Only allow incoming traffic from the subnet of the application servers on port 443.,,,1,radio,,,most restrictive approach is to allow only incoming connections from SG of EC2 instance on port 3306
To meet new data compliance requirements  a company needs to keep critical data durably stored and readily accessible for 7 years. Data that is more than 1 year old is considered archival data and must automatically be moved out of the Amazon Aurora MySQL DB cluster every week. On average  around 10 GB of new data is added to the database every month. A database specialist must choose the most operationally efficient solution to migrate the archival data to Amazon S3. Which solution meets these requirements?,A. Create a custom script that exports archival data from the DB cluster to Amazon S3 using a SQL view  then deletes the archival data from the DB cluste,B. Launch an Amazon EC2 instance with a weekly cron job to execute the custom script.,C. Configure an AWS Lambda function that exports archival data from the DB cluster to Amazon S3 using a SELECT INTO OUTFILE S3 statement  then deletes the archival data from the DB cluste,D. Schedule the Lambda function to run weekly using Amazon EventBridge (Amazon CloudWatch Events).,E. Configure two AWS Lambda functions: one that exports archival data from the DB cluster to Amazon S3 using the mysqldump utility  and another that deletes the archival data from the DB cluste,F. Schedule both Lambda functions to run weekly using Amazon EventBridge (Amazon CloudWatch Events). G. Use AWS Database Migration Service (AWS DMS) to continually export the archival data from the DB cluster to Amazon S3. Configure an AWS Data Pipeline process to run weekly that executes a custom SQL script to delete the archival data from the DB cluster.,2,radio,,,https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.SaveIntoS3.htm
A large retail company recently migrated its three-tier ecommerce applications to AWS. The company’s backend database is hosted on Amazon Aurora PostgreSQL. During peak times  users complain about longer page load times. A database specialist reviewed Amazon RDS Performance Insights and found a spike in IO:XactSync wait events. The SQL attached to the wait events are all single INSERT statements. How should this issue be resolved?,A. Modify the application to commit transactions in batches,B. Add a new Aurora Replica to the Aurora DB cluster.,C. Add an Amazon ElastiCache for Redis cluster and change the application to write through.,D. Change the Aurora DB cluster storage to Provisioned IOPS (PIOPS).,,,1,radio,,,"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Reference.html ""This wait most often arises when there is a very high rate of commit activity on the system. You can sometimes alleviate this wait by modifying applications to commit transactions in batches. "" https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/apg-waits.xactsync.html"
A company is planning to close for several days. A Database Specialist needs to stop all applications along with the DB instances to ensure employees do not have access to the systems during this time. All databases are running on Amazon RDS for MySQL. The Database Specialist wrote and executed a script to stop all the DB instances. When reviewing the logs  the Database Specialist found that Amazon RDS DB instances with read replicas did not stop. How should the Database Specialist edit the script to fix this issue?,A. Stop the source instances before stopping their read replicas,B. Delete each read replica before stopping its corresponding source instance,C. Stop the read replicas before stopping their source instances,D. Use the AWS CLI to stop each read replica and source instance at the same time,,,2,radio,,,"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html The following are some limitations to stopping and starting a DB instance: You can't stop a DB instance that has a read replica, or that is a read replica. So if you cant stop a db with a read replica  you have to delete the read replica first to then stop it??? https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.ReadReplicas.html#U"
A Database Specialist needs to define a database migration strategy to migrate an on-premises Oracle database to an Amazon Aurora MySQL DB cluster. The company requires near-zero downtime for the data migration. The solution must also be cost-effective. Which approach should the Database Specialist take?,A. Dump all the tables from the Oracle database into an Amazon S3 bucket using datapump (expdp). Run data transformations in AWS Glu,B. Load the data from the S3 bucket to the Aurora DB cluster.,C. Order an AWS Snowball appliance and copy the Oracle backup to the Snowball applianc,D. Once the Snowball data is delivered to Amazon S3  create a new Aurora DB cluste,E. Enable the S3 integration to migrate the data directly from Amazon S3 to Amazon RDS.,F. Use the AWS Schema Conversion Tool (AWS SCT) to help rewrite database objects to MySQL during the schema migratio G. Use AWS DMS to perform the full load and change data capture (CDC) tasks. H. Use AWS Server Migration Service (AWS SMS) to import the Oracle virtual machine image as an Amazon EC2 instanc I. Use the Oracle Logical Dump utility to migrate the Oracle data from Amazon EC2 to an Aurora DB cluster.,3,radio,,,https://aws.amazon.com/blogs/database/migrating-oracle-databases-with-near-zero-downtime-using-aws-dms/
A business is launching a new Amazon RDS for SQL Server database instance. The organization wishes to allow auditing of the SQL Server database. Which measures should a database professional perform in combination to achieve this requirement? (Select two.),A. Create a service-linked role for Amazon RDS that grants permissions for Amazon RDS to store audit logs on Amazon S3.,B. Set up a parameter group to configure an IAM role and an Amazon S3 bucket for audit log storage.Associate the parameter group with the DB instance.,C. Disable Multi-AZ on the DB instance  and then enable auditin,D. Enable Multi-AZ after auditing is enabled.,E. Disable automated backup on the DB instance  and then enable auditin,F. Enable automated backup after auditing is enabled. G. Set up an options group to configure an IAM role and an Amazon S3 bucket for audit log storage.Associate the options group with the DB instance.,1,radio,,,https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.Audit.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/security_iam_service-with-iam.html
A Database Specialist has migrated an on-premises Oracle database to Amazon Aurora PostgreSQL. The schema and the data have been migrated successfully. The on-premises database server was also being used to run database maintenance cron jobs written in Python to perform tasks including data purging and generating data exports. The logs for these jobs show that  most of the time  the jobs completed within 5 minutes  but a few jobs took up to 10 minutes to complete. These maintenance jobs need to be set up for Aurora PostgreSQL. How can the Database Specialist schedule these jobs so the setup requires minimal maintenance and provides high availability?,A. Create cron jobs on an Amazon EC2 instance to run the maintenance jobs following the required schedule.,B. Connect to the Aurora host and create cron jobs to run the maintenance jobs following the required schedule.,C. Create AWS Lambda functions to run the maintenance jobs and schedule them with Amazon CloudWatch Events.,D. Create the maintenance job using the Amazon CloudWatch job scheduling plugin.,,,3,radio,,,https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Scheduled-Rule.ht https://docs.aws.amazon.com/prescriptiveguidance/latest/patterns/schedule-jobs-for-amazon-rds-and-aurora-pos a job for data extraction or a job for data purging can easily be scheduled using cron. For these jobs  database credentials are typically either hard-coded or stored in a properties file. However  when you migrate to Amazon Relational Database Service (Amazon RDS) or Amazon Aurora PostgreSQL  you lose the ability to log in to the host instance to schedule cron jobs. This pattern describes how to use AWS Lambda and AWS Secrets Manager to schedule jobs for Amazon RDS and Aurora PostgreSQL databases after migration. https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html
A Database Specialist is creating a new Amazon Neptune DB cluster  and is attempting to load fata from Amazon S3 into the Neptune DB cluster using the Neptune bulk loader API. The Database Specialist receives the following error: “Unable to connect to s3 endpoint. Provided source = s3://mybucket/graphdata/ and region = us-east-1. Please verify your S3 configuration.” Which combination of actions should the Database Specialist take to troubleshoot the problem? (Choose two.),A. Check that Amazon S3 has an IAM role granting read access to Neptune,B. Check that an Amazon S3 VPC endpoint exists,C. Check that a Neptune VPC endpoint exists,D. Check that Amazon EC2 has an IAM role granting read access to Amazon S3,E. Check that Neptune has an IAM role granting read access to Amazon S3,,2,radio,,,
A Database Specialist must create a read replica to isolate read-only queries for an Amazon RDS for MySQL DB instance. Immediately after creating the read replica  users that query it report slow response times. What could be causing these slow response times?,A. New volumes created from snapshots load lazily in the background,B. Long-running statements on the master,C. Insufficient resources on the master,D. Overload of a single replication thread by excessive writes on the master,,,1,radio,,,snapshot is lazy loaded If the volume is accessed where the data is not loaded  the application accessing the volume encounters a higher latency than normal while the data gets loaded https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-ebs-fast-snapshot-restore-eliminates-need-for-p
Amazon Neptune is being used by a corporation as the graph database for one of its products. During an ETL procedure  the company's data science team produced enormous volumes of temporary data by unintentionally. The Neptune DB cluster extended its storage capacity automatically to handle the added data but the data science team erased the superfluous data. What should a database professional do to prevent incurring extra expenditures for cluster volume space that is not being used?,A. Take a snapshot of the cluster volum,B. Restore the snapshot in another cluster with a smaller volume size.,C. Use the AWS CLI to turn on automatic resizing of the cluster volume.,D. Export the cluster data into a new Neptune DB cluster.,E. Add a Neptune read replica to the cluste,F. Promote this replica as a new primary DB instanc G. Reset the storage space of the cluster.,3,radio,,,The only way to shrink the storage space used by your DB cluster when you have a large amount of unused allocated space is to export all the data in your graph and then reload it into a new DB cluster. Creating and restoring a snapshot does not reduce the amount of storage allocated for your DB cluster  because a snapshot retains the original image of the cluster's underlying storage.
A stock market analysis firm maintains two locations: one in the us-east-1 Region and another in the eu-west-2 Region. The business want to build an AWS database solution capable of providing rapid and accurate updates. Dashboards with advanced analytical queries are used to present data in the eu-west-2 office. Because the corporation will use these dashboards to make purchasing choices  they must have less than a second to obtain application data. Which solution satisfies these criteria and gives the MOST CURRENT dashboard?,A. Deploy an Amazon RDS DB instance in us-east-1 with a read replica instance in eu-west-2. Create an Amazon ElastiCache cluster in eu-west-2 to cache data from the read replica to generate the dashboards.,B. Use an Amazon DynamoDB global table in us-east-1 with replication into eu-west-2. Use multi-active replication to ensure that updates are quickly propagated to eu-west-2.,C. Use an Amazon Aurora global databas,D. Deploy the primary DB cluster in us-east-1. Deploy the secondary DB cluster in eu-west-2. Configure the dashboard application to read from the secondary cluster.,E. Deploy an Amazon RDS for MySQL DB instance in us-east-1 with a read replica instance in eu-west-2.Configure the dashboard application to read from the read replica.,,3,radio,,,Amazon Aurora global databases span multiple AWS Regions  enabling low latency global reads and providing fast recovery from the rare outage that might affect an entire AWS Region. An Aurora global database has a primary DB cluster in one Region  and up to five secondary DB clusters in different Regions. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html
A database specialist must create nightly backups of an Amazon DynamoDB table in a mission-critical workload as part of a disaster recovery strategy. Which backup methodology should the database specialist use to MINIMIZE management overhead?,A. Install the AWS CLI on an Amazon EC2 instanc,B. Write a CLI command that creates a backup of the DynamoDB tabl,C. Create a scheduled job or task that executes the command on a nightly basis.,D. Create an AWS Lambda function that creates a backup of the DynamoDB tabl,E. Create an Amazon CloudWatch Events rule that executes the Lambda function on a nightly basis.,F. Create a backup plan using AWS Backup  specify a backup frequency of every 24 hours  and give the plan a nightly backup window. G. Configure DynamoDB backup and restore for an on-demand backup frequency of every 24 hours.,3,radio,,,https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CreateBackup.html#:~:text=If%20you%2 https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/backuprestore_HowItWorks.html
A company is looking to migrate a 1 TB Oracle database from on-premises to an Amazon Aurora PostgreSQL DB cluster. The company’s Database Specialist discovered that the Oracle database is storing 100 GB of large binary objects (LOBs) across multiple tables. The Oracle database has a maximum LOB size of 500 MB with an average LOB size of 350 MB. The Database Specialist has chosen AWS DMS to migrate the data with the largest replication instances. How should the Database Specialist optimize the database migration using AWS DMS?,B. The Database Specialist has chosen AWS DMS to migrate the data with the largest replication instances. How should the Database Specialist optimize the database migration using AWS DMS?,A. Create a single task using full LOB mode with a LOB chunk size of 500 MB to migrate the data and LOBs together,B. Create two tasks: task1 with LOB tables using full LOB mode with a LOB chunk size of 500 MB and task2 without LOBs,C. Create two tasks: task1 with LOB tables using limited LOB mode with a maximum LOB size of 500 MB and task 2 without LOBs,D. Create a single task using limited LOB mode with a maximum LOB size of 500 MB to migrate data and LOBs together,,3,radio,,,
A worldwide digital advertising corporation collects browser information in order to provide targeted visitors with contextually relevant pictures  websites  and connections. A single page load may create many events  each of which must be kept separately. A single event may have a maximum size of 200 KB and an average size of 10 KB. Each page load requires a query of the user's browsing history in order to deliver suggestions for targeted advertising. The advertising corporation anticipates daily page views of more than 1 billion from people in the United States  Europe  Hong Kong  and India. The information structure differs according to the event. Additionally  browsing information must be written and read with a very low latency to guarantee that consumers have a positive viewing experience. Which database solution satisfies these criteria?,B. Each page load requires a query of the user's browsing history in order to deliver suggestions for targeted advertising. The advertising corporation anticipates daily page views of more than 1 billion from people in the United States  Europe  Hong Kong  and India. The information structure differs according to the event. Additionally  browsing information must be written and read with a very low latency to guarantee that consumers have a positive viewing experience. Which database solution satisfies these criteria?,A. Amazon DocumentDB,B. Amazon RDS Multi-AZ deployment,C. Amazon DynamoDB global table,D. Amazon Aurora Global Database,,3,radio,,,
A Database Specialist is constructing a new Amazon Neptune DB cluster and tries to load data from Amazon S3 using the Neptune bulk loader API. The Database Specialist is confronted with the following error message: €Unable to establish a connection to the s3 endpoint. The source URL is s3:/mybucket/graphdata/ and the region code is us-east-1. Kindly confirm your Configuration S3. Which of the following activities should the Database Specialist take to resolve the issue? (Select two.),A. Check that Amazon S3 has an IAM role granting read access to Neptune,B. Check that an Amazon S3 VPC endpoint exists,C. Check that a Neptune VPC endpoint exists,D. Check that Amazon EC2 has an IAM role granting read access to Amazon S3,E. Check that Neptune has an IAM role granting read access to Amazon S3,,2,radio,,,https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-tutorial-IAM.html https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html “An IAM role for the Neptune DB instance to assume that has an IAM policy that allows access to the data files in the S3 bucket. The policy must grant Read and List permissions.” “An Amazon S3 VPC endpoint. For more information  see the Creating an Amazon S3 VPC Endpoint section.”
A bank intends to utilize Amazon RDS to host a MySQL database instance. The database should be able to handle high-volume read requests with extremely few repeated queries. Which solution satisfies these criteria?,A. Create an Amazon ElastiCache cluste,B. Use a write-through strategy to populate the cache.,C. Create an Amazon ElastiCache cluste,D. Use a lazy loading strategy to populate the cache.,E. Change the DB instance to Multi-AZ with a standby instance in another AWS Region.,F. Create a read replica of the DB instanc G. Use the read replica to distribute the read traffic.,4,radio,,,
A company is using Amazon with Aurora Replicas for read-only workload scaling. A Database Specialist needs to split up two read-only applications so each application always connects to a dedicated replica. The Database Specialist wants to implement load balancing and high availability for the read-only applications. Which solution meets these requirements?,A. Use a specific instance endpoint for each replica and add the instance endpoint to each read-only application connection string.,B. Use reader endpoints for both the read-only workload applications.,C. Use a reader endpoint for one read-only application and use an instance endpoint for the other read-only application.,D. Use custom endpoints for the two read-only applications.,,,4,radio,,,https://aws.amazon.com/about-aws/whats-new/2018/11/amazon-aurora-simplifies-workload-management-with-c
A business just transitioned from an on-premises Oracle database to Amazon Aurora PostgreSQL. Following the move  the organization observed that every day around 3:00 PM  the application's response time is substantially slower. The firm has determined that the problem is with the database  not the application. Which set of procedures should the Database Specialist do to locate the erroneous PostgreSQL query most efficiently?,A. Create an Amazon CloudWatch dashboard to show the number of connections  CPU usage  and disk space consumptio,B. Watch these dashboards during the next slow period.,C. Launch an Amazon EC2 instance  and install and configure an open-source PostgreSQL monitoring tool that will run reports based on the output error logs.,D. Modify the logging database parameter to log all the queries related to locking in the database and then check the logs after the next slow period for this information.,E. Enable Amazon RDS Performance Insights on the PostgreSQL databas,F. Use the metrics to identify any queries that are related to spikes in the graph during the next slow period.,4,radio,,,"https://aws.amazon.com/blogs/database/optimizing-and-tuning-queries-in-amazon-rds-postgresql-based-on-nativ ""AWS recently released a feature called Amazon RDS Performance Insights  which provides an easy-to-understand dashboard for detecting performance problems in terms of load."" ""AWS recently released a feature called Amazon RDS Performance Insights which provides an easy-to-understand dashboard for detecting performance problems in terms of load."""
A company wants to migrate its Microsoft SQL Server Enterprise Edition database instance from on-premises to AWS. A deep review is performed and the AWS Schema Conversion Tool (AWS SCT) provides options for running this workload on Amazon RDS for SQL Server Enterprise Edition  Amazon RDS for SQL Server Standard Edition  Amazon Aurora MySQL  and Amazon Aurora PostgreSQL. The company does not want to use its own SQL server license and does not want to change from Microsoft SQL Server. What is the MOST cost-effective and operationally efficient solution?,A. Run SQL Server Enterprise Edition on Amazon EC2.,B. Run SQL Server Standard Edition on Amazon RDS.,C. Run SQL Server Enterprise Edition on Amazon RDS.,D. Run Amazon Aurora MySQL leveraging SQL Server on Linux compatibility libraries.,,,2,radio,,,This link seems to indicate that more information is required to determine if the Enterprise instance is a candidate for downgrading to Standard. https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/determine-whether-your-microsoft-sql-server https://calculator.aws/#/createCalculator/RDSSQLServer
In one AWS account  a business runs a two-tier ecommerce application. An Amazon RDS for MySQL Multi-AZ database instance serves as the application's backend. A developer removed the database instance in the production environment by accident. Although the organization recovers the database  the incident results in hours of outage and financial loss. Which combination of adjustments would reduce the likelihood that this error will occur again in the future? (Select three.),A. Grant least privilege to groups  IAM users  and roles.,B. Allow all users to restore a database from a backup.,C. Enable deletion protection on existing production DB instances.,D. Use an ACL policy to restrict users from DB instance deletion.,E. Enable AWS CloudTrail logging and Enhanced Monitoring.,,1,radio,,,
An online retail company is planning a multi-day flash sale that must support processing of up to 5 000 orders per second. The number of orders and exact schedule for the sale will vary each day. During the sale  approximately 10 000 concurrent users will look at the deals before buying items. Outside of the sale  the traffic volume is very low. The acceptable performance for read/write queries should be under 25 ms. Order items are about 2 KB in size and have a unique identifier. The company requires the most cost-effective solution that will automatically scale and is highly available. Which solution meets these requirements?,A. Amazon DynamoDB with on-demand capacity mode,B. Amazon Aurora with one writer node and an Aurora Replica with the parallel query feature enabled,C. Amazon DynamoDB with provisioned capacity mode with 5 000 write capacity units (WCUs) and 10 000 read capacity units (RCUs),D. Amazon Aurora with one writer node and two cross-Region Aurora Replicas,,,1,radio,,,The number of orders and exact schedule for the sale will vary each day. During the sale  approximately 10 000 concurrent users will look at the deals before buying items. Outside of the sale  the traffic volume is very low ==> Setting provisioning DynamoDB fix read 5000/write 10000 with will waste the resource when the traffic is low. It is not cost-effective.
